{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-s5Yb152pi2"
   },
   "source": [
    "# convnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nx-WsjiR23jz"
   },
   "source": [
    "Instantiating a small convnet\n",
    "\n",
    "a convnet takes as input tensors of shape (image_height, image_width,\n",
    "image_channels), not including the batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SaXwkTMG1mcW"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1734180032601,
     "user": {
      "displayName": "Hayder Chakroun",
      "userId": "02691572659768110935"
     },
     "user_tz": -60
    },
    "id": "dtqJPLAt3Pt9",
    "outputId": "13842014-73b3-4c88-ac42-e0bd584b3126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 13, 13, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                11530     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104,202\n",
      "Trainable params: 104,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfK6198n7Ncz"
   },
   "source": [
    "Because we’re doing 10-way classification with a\n",
    "softmax output, we’ll use the categorical crossentropy loss, and because our labels are\n",
    "integers, we’ll use the sparse version, sparse_categorical_crossentropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2gyQ4rn7O9r"
   },
   "source": [
    "Training the convnet on MNIST images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 215742,
     "status": "ok",
     "timestamp": 1734180248337,
     "user": {
      "displayName": "Hayder Chakroun",
      "userId": "02691572659768110935"
     },
     "user_tz": -60
    },
    "id": "Ci4dq9eV3P6d",
    "outputId": "95e49947-6d18-4b89-80a2-43d3d4730e42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 13s 13ms/step - loss: 0.1571 - accuracy: 0.9513\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.0442 - accuracy: 0.9864\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.0312 - accuracy: 0.9901\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.0232 - accuracy: 0.9926\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.0185 - accuracy: 0.9946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b944e00f70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype(\"float32\") / 255\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "loss=\"sparse_categorical_crossentropy\",\n",
    "metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvf-AzpP7XJr"
   },
   "source": [
    "Evaluating the convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2228,
     "status": "ok",
     "timestamp": 1734180250560,
     "user": {
      "displayName": "Hayder Chakroun",
      "userId": "02691572659768110935"
     },
     "user_tz": -60
    },
    "id": "iOF30lkk7Y79",
    "outputId": "d44288bd-2563-4942-fbf2-14a487e31812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.0277 - accuracy: 0.9914\n",
      "Test accuracy: 0.991\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXnaO5Nn7sud"
   },
   "source": [
    "##The convolution operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSOjTwGfZyww"
   },
   "source": [
    "In Conv2D layers, padding is configurable via the padding argument, which takes two\n",
    "values: \"valid\", which means no padding (only valid window locations will be used),\n",
    "and \"same\", which means “pad in such a way as to have an output with the same width\n",
    "and height as the input.” The padding argument defaults to \"valid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXi5EzHRZ9t9"
   },
   "source": [
    "Using stride 2 means the width and height of the feature map are downsampled by a\n",
    "factor of 2 (in addition to any changes induced by border effects)\n",
    "\n",
    "Strided convolutions are rarely used in classification models, but they come in handy for some types of\n",
    "models,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDJC8klgaUJM"
   },
   "source": [
    " the role of max pooling: to aggressively downsample feature maps, much like\n",
    "strided convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eODflMMKauik"
   },
   "source": [
    "A big difference from convolution is that max pooling is usually done\n",
    "with 2 × 2 windows and stride 2, in order to downsample the feature maps by a factor of 2. On the other hand, convolution is typically done with 3 × 3 windows and no\n",
    "stride (stride 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKfYhMrNa3-8"
   },
   "source": [
    "An incorrectly structured convnet missing its max-pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "inHwt9b-a5M1"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1734180250560,
     "user": {
      "displayName": "Hayder Chakroun",
      "userId": "02691572659768110935"
     },
     "user_tz": -60
    },
    "id": "ofu25wlEa8Vc",
    "outputId": "8221624b-43ed-4302-9620-fe50b6e9d1b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 22, 22, 128)       73856     \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 61952)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                619530    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 712,202\n",
      "Trainable params: 712,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_no_max_pool.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xr0sMNQbYxM"
   },
   "source": [
    "The final feature map has 22 × 22 × 128 = 61,952 total coefficients per sample.\n",
    "This is huge. When you flatten it to stick a Dense layer of size 10 on top, that\n",
    "layer would have over half a million parameters. This is far too large for such a\n",
    "small model and would result in intense overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoQrssDMbtYU"
   },
   "source": [
    "In short, the reason to use downsampling is to reduce the number of feature-map\n",
    "coefficients to process, as well as to induce spatial-filter hierarchies by making successive convolution layers look at increasingly large windows (in terms of the fraction of\n",
    "the original input they cover)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1W0iWxocfad"
   },
   "source": [
    "The most reasonable subsampling strategy is to first produce dense maps of\n",
    "features (via unstrided convolutions) and then look at the maximal activation of the\n",
    "features over small patches, rather than looking at sparser windows of the inputs (via\n",
    "strided convolutions) or averaging input patches, which could cause you to miss or\n",
    "dilute feature-presence information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyKugu4icuWt"
   },
   "source": [
    "##Training a convnet from scratch on a small dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJdSO4S-dGne"
   },
   "source": [
    "these three strategies—training a small model from scratch,\n",
    "doing feature extraction using a pretrained model, and fine-tuning a pretrainedmodel—will constitute your future toolbox for tackling the problem of performing\n",
    "image classification with small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TBsqBCBeQWE"
   },
   "source": [
    "First, you need to create a Kaggle API key and download it to your local machine. Just\n",
    "navigate to the Kaggle website in a web browser, log in, and go to the My Account\n",
    "page. In your account settings, you’ll find an API section. Clicking the Create New API\n",
    "Token button will generate a kaggle.json key file and will download it to your machine.\n",
    "Second, go to your Colab notebook, and upload the API’s key JSON file to your Colab\n",
    "session by running the following code in a notebook cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "executionInfo": {
     "elapsed": 232113,
     "status": "ok",
     "timestamp": 1734180482662,
     "user": {
      "displayName": "Hayder Chakroun",
      "userId": "02691572659768110935"
     },
     "user_tz": -60
    },
    "id": "HVmEO-b1cu-_",
    "outputId": "db23601e-144f-4356-93dd-d476bb11da06"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gnme_Q8EeVrm"
   },
   "outputs": [],
   "source": [
    "#mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PUyr8tIceaWd"
   },
   "outputs": [],
   "source": [
    "#cp kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YSPJCQpleeJe"
   },
   "outputs": [],
   "source": [
    "#!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4840,
     "status": "ok",
     "timestamp": 1734180487775,
     "user": {
      "displayName": "Hayder Chakroun",
      "userId": "02691572659768110935"
     },
     "user_tz": -60
    },
    "id": "KyP2l2Q5ejT9",
    "outputId": "39da8fa2-4f44-494c-eb74-f29dfd120c33"
   },
   "outputs": [],
   "source": [
    "#!kaggle competitions download -c dogs-vs-cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRKRbGpGe7-E"
   },
   "source": [
    "Finally, create a ~/.kaggle folder (mkdir ~/.kaggle), and copy the key file to it\n",
    "(cp kaggle.json ~/.kaggle/). As a security best practice, you should also make\n",
    "sure that the file is only readable by the current user, yourself (chmod 600):\n",
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "You can now download the data we’re about to use:\n",
    "!kaggle competitions download -c dogs-vs-cats\n",
    "The first time you try to download the data, you may get a “403 Forbidden” error.\n",
    "That’s because you need to accept the terms associated with the dataset before you\n",
    "download it—you’ll have to go to www.kaggle.com/c/dogs-vs-cats/rules (while\n",
    "logged into your Kaggle account) and click the I Understand and Accept button. You\n",
    "only need to do this once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLYfYZ3Ze-dE"
   },
   "source": [
    "Finally, the training data is a compressed file named train.zip. Make sure you uncompress it (unzip) silently (-qq):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Y41_01VhfBRc"
   },
   "outputs": [],
   "source": [
    "#!unzip -qq dogs-vs-cats.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "aMDijVIdiwOm"
   },
   "outputs": [],
   "source": [
    "#!unzip -qq train.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTKP4m8Who_G"
   },
   "source": [
    "Copying images to training, validation, and test directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aEXdfA0zfDlV"
   },
   "outputs": [],
   "source": [
    "import os, shutil, pathlib\n",
    "original_dir = pathlib.Path(\"train\")\n",
    "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "kR1lSqYmh3pk"
   },
   "outputs": [],
   "source": [
    "def make_subset(subset_name, start_index, end_index):\n",
    "  for category in (\"cat\", \"dog\"):\n",
    "    dir = new_base_dir / subset_name / category\n",
    "    os.makedirs(dir)\n",
    "    fnames = [f\"{category}.{i}.jpg\"\n",
    "      for i in range(start_index, end_index)]\n",
    "    for fname in fnames:\n",
    "      shutil.copyfile(src=original_dir / fname,\n",
    "        dst=dir / fname)\n",
    "#make_subset(\"train\", start_index=0, end_index=1000)\n",
    "#make_subset(\"validation\", start_index=1000, end_index=1500)\n",
    "#make_subset(\"test\", start_index=1500, end_index=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moKa3JgPN1ca"
   },
   "source": [
    "Here, because we start from inputs of size 180 pixels × 180 pixels (a somewhat arbitrary\n",
    "choice), we end up with feature maps of size 7 × 7 just before the Flatten layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67h5Cr8PN7fy"
   },
   "source": [
    "NOTE The depth of the feature maps progressively increases in the model\n",
    "(from 32 to 256), whereas the size of the feature maps decreases (from 180 ×\n",
    "180 to 7 × 7). This is a pattern you’ll see in almost all convnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6F9TqeXOA3S"
   },
   "source": [
    "Because we’re looking at a binary-classification problem, we’ll end the model with a\n",
    "single unit (a Dense layer of size 1) and a sigmoid activation. This unit will encode the\n",
    "probability that the model is looking at one class or the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFAT_8d0OE_S"
   },
   "source": [
    "Instantiating a small convnet for dogs vs. cats classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "MIufMhABOG57"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1734180701264,
     "user": {
      "displayName": "Hayder Chakroun",
      "userId": "02691572659768110935"
     },
     "user_tz": -60
    },
    "id": "SqvUVuglOWCb",
    "outputId": "864709b0-6bc1-441a-caf2-1fdb2c1e6ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 180, 180, 3)]     0         \n",
      "                                                                 \n",
      " rescaling (Rescaling)       (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 178, 178, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 89, 89, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 87, 87, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 43, 43, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 41, 41, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 20, 20, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 18, 18, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 9, 9, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 7, 7, 256)         590080    \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 12545     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 991,041\n",
      "Trainable params: 991,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRqclassOlcM"
   },
   "source": [
    "For the compilation step, we’ll go with the RMSprop optimizer, as usual. Because we\n",
    "ended the model with a single sigmoid unit, we’ll use binary crossentropy as the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnCuSNxOOm6s"
   },
   "source": [
    "Configuring the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "3-jupIdOOoL6"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "optimizer=\"rmsprop\",\n",
    "metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHZj_sNaOqyC"
   },
   "source": [
    "###Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exzi23j7OzJk"
   },
   "source": [
    "As you know by now, data should be formatted into appropriately preprocessed floatingpoint tensors before being fed into the model. Currently, the data sits on a drive as\n",
    "JPEG files, so the steps for getting it into the model are roughly as follows:\n",
    "1 Read the picture files.\n",
    "2 Decode the JPEG content to RGB grids of pixels.\n",
    "3 Convert these into floating-point tensors.\n",
    "4 Resize them to a shared size (we’ll use 180 × 180).\n",
    "5 Pack them into batches (we’ll use batches of 32 images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYoFFtCUPDpj"
   },
   "source": [
    "Calling image_dataset_from_directory(directory)  will create and return a\n",
    "tf.data.Dataset object configured to read these files, shuffle them, decode them to\n",
    "tensors, resize them to a shared size, and pack them into batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SVhmw9fPKGZ"
   },
   "source": [
    "Using image_dataset_from_directory to read images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1734180936921,
     "user": {
      "displayName": "Hayder Chakroun",
      "userId": "02691572659768110935"
     },
     "user_tz": -60
    },
    "id": "8KJ28te-PL-r",
    "outputId": "a0628073-7b44-439c-e5ea-2e6c0ce84556"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 2 classes.\n",
      "Found 1000 files belonging to 2 classes.\n",
      "Found 2000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "train_dataset = image_dataset_from_directory(\n",
    "new_base_dir / \"train\",\n",
    "image_size=(180, 180),\n",
    "batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "new_base_dir / \"validation\",\n",
    "image_size=(180, 180),\n",
    "batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "new_base_dir / \"test\",\n",
    "image_size=(180, 180),\n",
    "batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvlHUGjDTANK"
   },
   "source": [
    "Displaying the shapes of the data and labels yielded by the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 898,
     "status": "ok",
     "timestamp": 1734181943497,
     "user": {
      "displayName": "Hayder Chakroun",
      "userId": "02691572659768110935"
     },
     "user_tz": -60
    },
    "id": "8dxCFdlaTEAl",
    "outputId": "92452db4-11b5-4887-9cc3-a19fb6170157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (32, 180, 180, 3)\n",
      "labels batch shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_dataset:\n",
    "  print(\"data batch shape:\", data_batch.shape)\n",
    "  print(\"labels batch shape:\", labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AO7CJKZTnFK"
   },
   "source": [
    "Fitting the model using a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_9-UbZ6TozK",
    "outputId": "994df9ab-0f85-4e51-e967-317ad992f07c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_3/conv2d_9/Relu' defined at (most recent call last):\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\GIGABYTE\\AppData\\Local\\Temp\\ipykernel_2880\\1018272495.py\", line 7, in <module>\n      history = model.fit(\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 314, in call\n      return self.activation(outputs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\activations.py\", line 317, in relu\n      return backend.relu(\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend.py\", line 5366, in relu\n      x = tf.nn.relu(x)\nNode: 'model_3/conv2d_9/Relu'\nNo algorithm worked!  Error messages:\n  Profiling failure on CUDNN engine 1#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 1: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 0#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 0: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 2#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 126277120 bytes.\n  Profiling failure on CUDNN engine 2: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 126277120 bytes.\n  Profiling failure on CUDNN engine 4#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 557845888 bytes.\n  Profiling failure on CUDNN engine 4: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 557845888 bytes.\n  Profiling failure on CUDNN engine 6#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16798208 bytes.\n  Profiling failure on CUDNN engine 6: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16798208 bytes.\n  Profiling failure on CUDNN engine 5#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 26943488 bytes.\n  Profiling failure on CUDNN engine 5: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 26943488 bytes.\n  Profiling failure on CUDNN engine 7#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 343383040 bytes.\n  Profiling failure on CUDNN engine 7: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 343383040 bytes.\n\t [[{{node model_3/conv2d_9/Relu}}]] [Op:__inference_train_function_23549]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[0;32m      3\u001b[0m filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvnet_from_scratch.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m ]\n\u001b[1;32m----> 7\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Graph execution error:\n\nDetected at node 'model_3/conv2d_9/Relu' defined at (most recent call last):\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\GIGABYTE\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\GIGABYTE\\AppData\\Local\\Temp\\ipykernel_2880\\1018272495.py\", line 7, in <module>\n      history = model.fit(\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 314, in call\n      return self.activation(outputs)\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\activations.py\", line 317, in relu\n      return backend.relu(\n    File \"c:\\Users\\GIGABYTE\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend.py\", line 5366, in relu\n      x = tf.nn.relu(x)\nNode: 'model_3/conv2d_9/Relu'\nNo algorithm worked!  Error messages:\n  Profiling failure on CUDNN engine 1#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 1: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 0#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 0: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes.\n  Profiling failure on CUDNN engine 2#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 126277120 bytes.\n  Profiling failure on CUDNN engine 2: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 126277120 bytes.\n  Profiling failure on CUDNN engine 4#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 557845888 bytes.\n  Profiling failure on CUDNN engine 4: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 557845888 bytes.\n  Profiling failure on CUDNN engine 6#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16798208 bytes.\n  Profiling failure on CUDNN engine 6: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16798208 bytes.\n  Profiling failure on CUDNN engine 5#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 26943488 bytes.\n  Profiling failure on CUDNN engine 5: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 26943488 bytes.\n  Profiling failure on CUDNN engine 7#TC: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 343383040 bytes.\n  Profiling failure on CUDNN engine 7: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 343383040 bytes.\n\t [[{{node model_3/conv2d_9/Relu}}]] [Op:__inference_train_function_23549]"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\n",
    "filepath=\"convnet_from_scratch.keras\",\n",
    "save_best_only=True,\n",
    "monitor=\"val_loss\")\n",
    "]\n",
    "history = model.fit(\n",
    "train_dataset,\n",
    "epochs=30,\n",
    "validation_data=validation_dataset,\n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOJOPFyvT4RG"
   },
   "source": [
    "Displaying curves of loss and accuracy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 319,
     "status": "error",
     "timestamp": 1734185447046,
     "user": {
      "displayName": "Hayder Chakroun",
      "userId": "02691572659768110935"
     },
     "user_tz": -60
    },
    "id": "Q9I_kne3T6Cr",
    "outputId": "0b516ebc-556a-4fc5-fc5d-d8db86ce98e3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9cfebfbf608d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HK1UNPeCUoHM"
   },
   "source": [
    " Evaluating the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "error",
     "timestamp": 1734185453675,
     "user": {
      "displayName": "Hayder Chakroun",
      "userId": "02691572659768110935"
     },
     "user_tz": -60
    },
    "id": "2wU8aQkIUpg-",
    "outputId": "773c6f45-57dd-4dbc-f11e-6daee039b5f0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-653ec56a21b1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"convnet_from_scratch.keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test accuracy: {test_acc:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "test_model = keras.models.load_model(\"convnet_from_scratch.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EX6cukfU6AS"
   },
   "source": [
    "Because we have relatively few training samples (2,000), overfitting will be our\n",
    "number one concern. You already know about a number of techniques that can help\n",
    "mitigate overfitting, such as dropout and weight decay (L2 regularization). We’re now\n",
    "going to work with a new one, specific to computer vision and used almost universally\n",
    "when processing images with deep learning models: data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_u8PYkteU-xp"
   },
   "source": [
    "###Using data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0vwpiIIV2Bi"
   },
   "source": [
    "Define a data augmentation stage to add to an image model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXUpa99DU_pW"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "[\n",
    "layers.RandomFlip(\"horizontal\"),\n",
    "layers.RandomRotation(0.1),\n",
    "layers.RandomZoom(0.2),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "En2ibWqkXMep"
   },
   "source": [
    "Displaying some randomly augmented training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8GR4Oj0XLNe"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_dataset.take(1):\n",
    "for i in range(9):\n",
    "augmented_images = data_augmentation(images)\n",
    "ax = plt.subplot(3, 3, i + 1)\n",
    "plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yszu9E_jZh2z"
   },
   "source": [
    "this may not be enough\n",
    "to completely get rid of overfitting. To further fight overfitting, we’ll also add a Dropout\n",
    "layer to our model right before the densely connected classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omirxgrZZpKr"
   },
   "source": [
    "ust\n",
    "like Dropout, they’re inactive during inference (when we call predict() or evaluate()).\n",
    "During evaluation, our model will behave just the same as when it did not include\n",
    "data augmentation and dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sEiXrLhZsMD"
   },
   "source": [
    "Defining a new convnet that includes image augmentation and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eKz4oEfZvbb"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "optimizer=\"rmsprop\",\n",
    "metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RdSnANBZ3SJ"
   },
   "source": [
    "Training the regularized convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ah8TC66DZ5ld"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\n",
    "filepath=\"convnet_from_scratch_with_augmentation.keras\",\n",
    "save_best_only=True,\n",
    "monitor=\"val_loss\")\n",
    "]\n",
    "history = model.fit(\n",
    "train_dataset,\n",
    "epochs=100,\n",
    "validation_data=validation_dataset,\n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwuE4pBZaJ9k"
   },
   "source": [
    "The validation accuracy ends up consistently in the 80–85% range—\n",
    "a big improvement over our first try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vt33_H3-aLeU"
   },
   "source": [
    "Evaluating the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJTDCoaGaMj9"
   },
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model(\n",
    "\"convnet_from_scratch_with_augmentation.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48SQaEAraoXi"
   },
   "source": [
    "By further tuning the model’s configuration (such as the number of filters per\n",
    "convolution layer, or the number of layers in the model), we might be able to get an\n",
    "even better accuracy, likely up to 90%. But it would prove difficult to go any higher\n",
    "just by training our own convnet from scratch, because we have so little data to work\n",
    "with. As a next step to improve our accuracy on this problem, we’ll have to use a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-yytRBVaqE2"
   },
   "source": [
    "##Leveraging a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxGhn3hebeKs"
   },
   "source": [
    "We’ll use the VGG16 architecture, developed by Karen Simonyan and Andrew\n",
    "Zisserman in 2014.1 Although it’s an older model, far from the current state of the art\n",
    "and somewhat heavier than many other recent models, I chose it because its architecture is similar to what you’re already familiar with, and it’s easy to understand without\n",
    "introducing any new concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Drma2G8gbn5C"
   },
   "source": [
    "There are two ways to use a pretrained model: feature extraction and fine-tuning.\n",
    "We’ll cover both of them. Let’s start with feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeDKMN3sbp_F"
   },
   "source": [
    "###Feature extraction with a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WseUNhUDbvMd"
   },
   "source": [
    "Feature extraction consists of using the representations learned by a previously\n",
    "trained model to extract interesting features from new samples. These features are\n",
    "then run through a new classifier, which is trained from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5EvGhmvcDub"
   },
   "source": [
    "convnets used for image classification comprise two parts:\n",
    "they start with a series of pooling and convolution layers, and they end with a densely\n",
    "connected classifier. The first part is called the convolutional base of the model. In the\n",
    "case of convnets, feature extraction consists of taking the convolutional base of a previously trained network, running the new data through it, and training a new classifier\n",
    "on top of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bf99hTblcUHN"
   },
   "source": [
    "Why only reuse the convolutional base? Could we reuse the densely connected\n",
    "classifier as well? In general, doing so should be avoided. The reason is that the representations learned by the convolutional base are likely to be more generic and, therefore, more reusable: the feature maps of a convnet are presence maps of generic\n",
    "concepts over a picture, which are likely to be useful regardless of the computer vision\n",
    "problem at hand. But the representations learned by the classifier will necessarily be\n",
    "specific to the set of classes on which the model was trained—they will only contain\n",
    "information about the presence probability of this or that class in the entire picture.\n",
    "Additionally, representations found in densely connected layers no longer contain any information about where objects are located in the input image; these layers get rid of\n",
    "the notion of space, whereas the object location is still described by convolutional feature maps. For problems where object location matters, densely connected features\n",
    "are largely useless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgw-STgdc0KL"
   },
   "source": [
    "In this case, because the ImageNet class set contains multiple dog and cat\n",
    "classes, it’s likely to be beneficial to reuse the information contained in the densely\n",
    "connected layers of the original model. But we’ll choose not to, in order to cover\n",
    "the more general case where the class set of the new problem doesn’t overlap the\n",
    "class set of the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDBzp0EHdFQ1"
   },
   "source": [
    "Let’s put this into practice by using the convolutional base of the VGG16 network, trained on ImageNet, to extract interesting features from cat and dog images, and then train a dogs-versus-cats classifier on top of\n",
    "these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlrHG1pff4ex"
   },
   "source": [
    "**Instantiating the VGG16 convolutional base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNBeyLqGcZj5"
   },
   "outputs": [],
   "source": [
    "conv_base = keras.applications.vgg16.VGG16(\n",
    "weights=\"imagenet\",\n",
    "include_top=False,\n",
    "input_shape=(180, 180, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PihW8hiegIUs"
   },
   "source": [
    "We pass three arguments to the constructor:\n",
    " weights specifies the weight checkpoint from which to initialize the model.\n",
    " include_top refers to including (or not) the densely connected classifier on\n",
    "top of the network. By default, this densely connected classifier corresponds to\n",
    "the 1,000 classes from ImageNet. Because we intend to use our own densely\n",
    "connected classifier (with only two classes: cat and dog), we don’t need to\n",
    "include it.\n",
    " input_shape is the shape of the image tensors that we’ll feed to the network.\n",
    "This argument is purely optional: if we don’t pass it, the network will be able to\n",
    "process inputs of any size. Here we pass it so that we can visualize (in the following summary) how the size of the feature maps shrinks with each new convolution and pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGwSjmekgL8E"
   },
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j37P577t4LNq"
   },
   "source": [
    "FAST FEATURE EXTRACTION WITHOUT DATA AUGMENTATION\n",
    "We’ll start by extracting features as NumPy arrays by calling the predict() method of\n",
    "the conv_base model on our training, validation, and testing datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToqUPyym4ODB"
   },
   "source": [
    "Extracting the VGG16 features and corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0V1Fni84PRS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_features_and_labels(dataset):\n",
    "  all_features = []\n",
    "  all_labels = []\n",
    "  for images, labels in dataset:\n",
    "    preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
    "    features = conv_base.predict(preprocessed_images)\n",
    "    all_features.append(features)\n",
    "    all_labels.append(labels)\n",
    "  return np.concatenate(all_features), np.concatenate(all_labels)\n",
    "\n",
    "train_features, train_labels = get_features_and_labels(train_dataset)\n",
    "val_features, val_labels = get_features_and_labels(validation_dataset)\n",
    "test_features, test_labels = get_features_and_labels(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJwbdbWI4oOZ"
   },
   "source": [
    "the VGG16 model expects\n",
    "inputs that are preprocessed with the function keras.applications.vgg16.preprocess_input, which scales pixel values to an appropriate range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEHzE0IW4r4c"
   },
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCwyPULg41rh"
   },
   "source": [
    "Defining and training the densely connected classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70LiXxvW45QZ"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(5, 5, 512))\n",
    "x = layers.Flatten()(inputs)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "optimizer=\"rmsprop\",\n",
    "metrics=[\"accuracy\"])\n",
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\n",
    "filepath=\"feature_extraction.keras\",\n",
    "save_best_only=True,\n",
    "monitor=\"val_loss\")\n",
    "]\n",
    "history = model.fit(\n",
    "train_features, train_labels,\n",
    "epochs=20,\n",
    "validation_data=(val_features, val_labels),\n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fe0wwF1R5ILc"
   },
   "source": [
    "Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "invYY1ha5Jhp"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBSdg5Ec5Tnw"
   },
   "source": [
    "the plots also indicate that we’re overfitting almost from the start—\n",
    "despite using dropout with a fairly large rate. That’s because this technique doesn’t\n",
    "use data augmentation, which is essential for preventing overfitting with small image\n",
    "datasets.\n",
    "\n",
    "**FEATURE EXTRACTION TOGETHER WITH DATA AUGMENTATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lejPVW7n5d1x"
   },
   "source": [
    "Now let’s review the second technique I mentioned for doing feature extraction,\n",
    "which is much slower and more expensive, but which allows us to use data augmentation during training: creating a model that chains the conv_base with a new dense\n",
    "classifier, and training it end to end on the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urVLJ73v5nAY"
   },
   "source": [
    "In order to do this, we will first freeze the convolutional base.\n",
    "\n",
    "In Keras, we freeze a layer or model by setting its trainable attribute to False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bByNDGuv5qqY"
   },
   "source": [
    " Instantiating and freezing the VGG16 convolutional base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPrYuLk95t7Y"
   },
   "outputs": [],
   "source": [
    "conv_base = keras.applications.vgg16.VGG16(\n",
    "weights=\"imagenet\",\n",
    "include_top=False)\n",
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ctaP5MN5yQn"
   },
   "source": [
    "Setting trainable to False empties the list of trainable weights of the layer or model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1yN_wU_5zhB"
   },
   "source": [
    "Printing the list of trainable weights before and after freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuuqdNTt50nx"
   },
   "outputs": [],
   "source": [
    "conv_base.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkhAAqFZ527v"
   },
   "outputs": [],
   "source": [
    "print(\"This is the number of trainable weights \"\n",
    "\"before freezing the conv base:\", len(conv_base.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Vtk75aw55l4"
   },
   "outputs": [],
   "source": [
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEWT71Pj56yH"
   },
   "outputs": [],
   "source": [
    "print(\"This is the number of trainable weights \"\n",
    "\"after freezing the conv base:\", len(conv_base.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSch05246FyX"
   },
   "source": [
    "we can create a new model that chains together\n",
    "1 A data augmentation stage\n",
    "2 Our frozen convolutional base\n",
    "3 A dense classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_jOFMtj6G1I"
   },
   "source": [
    "Adding a data augmentation stage and a classifier to the convolutional base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1C7y10X26IXg"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "[\n",
    "layers.RandomFlip(\"horizontal\"),\n",
    "layers.RandomRotation(0.1),\n",
    "layers.RandomZoom(0.2),\n",
    "]\n",
    ")\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = keras.applications.vgg16.preprocess_input(x)\n",
    "x = conv_base(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "optimizer=\"rmsprop\",\n",
    "metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIyoezWP6pm_"
   },
   "source": [
    "This technique is expensive enough that you should only attempt it if\n",
    "you have access to a GPU (such as the free GPU available in Colab)—it’s\n",
    "intractable on CPU. If you can’t run your code on GPU, then the previous\n",
    "technique is the way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CONgyvmC6rdH"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\n",
    "filepath=\"feature_extraction_with_data_augmentation.keras\",\n",
    "save_best_only=True,\n",
    "monitor=\"val_loss\")\n",
    "]\n",
    "history = model.fit(\n",
    "train_dataset,\n",
    "epochs=50,\n",
    "validation_data=validation_dataset,\n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oFMm85k7Faf"
   },
   "source": [
    "Let’s plot the results again (see figure 8.14). As you can see, we reach a validation\n",
    "accuracy of over 98%. This is a strong improvement over the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSDNnFEK7H4G"
   },
   "source": [
    "Let’s check the test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsB5XXRO7LNW"
   },
   "source": [
    "Evaluating the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgyh8Cbs7MYO"
   },
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model(\n",
    "\"feature_extraction_with_data_augmentation.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1TlM4Cy7YFG"
   },
   "source": [
    "###Fine-tuning a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cm-jsBMZ9uQd"
   },
   "source": [
    "Fine-tuning consists of unfreezing\n",
    "a few of the top layers of a frozen model base used\n",
    "for feature extraction, and jointly training both the\n",
    "newly added part of the model (in this case, the\n",
    "fully connected classifier) and these top layers. This\n",
    "is called fine-tuning because it slightly adjusts the\n",
    "more abstract representations of the model being\n",
    "reused in order to make them more relevant for the\n",
    "problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpbJVIq99_jj"
   },
   "source": [
    "the steps for fine-tuning a network are as follows:\n",
    "1 Add our custom network on top of an\n",
    "already-trained base network.\n",
    "2 Freeze the base network.\n",
    "3 Train the part we added.\n",
    "4 Unfreeze some layers in the base network.\n",
    "(Note that you should not unfreeze “batch\n",
    "normalization” layers, which are not relevant\n",
    "here since there are no such layers in VGG16.\n",
    "Batch normalization and its impact on finetuning is explained in the next chapter.)\n",
    "5 Jointly train both these layers and the part we\n",
    "added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1uWn5bF-JqL"
   },
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Va1eNQxO-gq8"
   },
   "source": [
    "Why not fine-tune more layers? Why not fine-tune the entire convolutional base?\n",
    "You could. But you need to consider the following:\n",
    " Earlier layers in the convolutional base encode more generic, reusable features,\n",
    "whereas layers higher up encode more specialized features. It’s more useful to\n",
    "fine-tune the more specialized features, because these are the ones that need\n",
    "to be repurposed on your new problem. There would be fast-decreasing returns\n",
    "in fine-tuning lower layers.\n",
    " The more parameters you’re training, the more you’re at risk of overfitting.\n",
    "The convolutional base has 15 million parameters, so it would be risky to\n",
    "attempt to train it on your small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHWZxj6l-lXb"
   },
   "source": [
    "Freezing all layers until the fourth from the last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32eXRkET7Yzp"
   },
   "outputs": [],
   "source": [
    "onv_base.trainable = True\n",
    "for layer in conv_base.layers[:-4]:\n",
    "layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OX5aqlh-wWU"
   },
   "source": [
    "Now we can begin fine-tuning the model. We’ll do this with the RMSprop optimizer,\n",
    "using a very low learning rate. The reason for using a low learning rate is that we want to\n",
    "limit the magnitude of the modifications we make to the representations of the three\n",
    "layers we’re fine-tuning. Updates that are too large may harm these representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5c7x_nV-xVz"
   },
   "source": [
    "Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqEei1p9-0tD"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n",
    "metrics=[\"accuracy\"])\n",
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\n",
    "filepath=\"fine_tuning.keras\",\n",
    "save_best_only=True,\n",
    "monitor=\"val_loss\")\n",
    "]\n",
    "history = model.fit(\n",
    "train_dataset,\n",
    "epochs=30,\n",
    "validation_data=validation_dataset,\n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVcuijCA-39L"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"fine_tuning.keras\")\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2bCpEaR_Md3"
   },
   "source": [
    "On the positive side, by leveraging modern deep learning techniques, we managed\n",
    "to reach this result using only a small fraction of the training data that was available\n",
    "for the competition (about 10%). There is a huge difference between being able to\n",
    "train on 20,000 samples compared to 2,000 samples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7KLth8g_Y8z"
   },
   "source": [
    "Convnets are the best type of machine learning models for computer vision\n",
    "tasks. It’s possible to train one from scratch even on a very small dataset, with\n",
    "decent results.\n",
    " Convnets work by learning a hierarchy of modular patterns and concepts to\n",
    "represent the visual world.\n",
    " On a small dataset, overfitting will be the main issue. Data augmentation is a\n",
    "powerful way to fight overfitting when you’re working with image data.\n",
    " It’s easy to reuse an existing convnet on a new dataset via feature extraction.\n",
    "This is a valuable technique for working with small image datasets.\n",
    " As a complement to feature extraction, you can use fine-tuning, which adapts to\n",
    "a new problem some of the representations previously learned by an existing\n",
    "model. This pushes performance a bit further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ot8Kkbt7AEGP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOUoz8E9XCHs68OcRS7YRCv",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
