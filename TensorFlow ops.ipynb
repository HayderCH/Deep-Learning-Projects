{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","authorship_tag":"ABX9TyP74jj54Ux61Q5xVAZbckGX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["##The Sequential class"],"metadata":{"id":"daQPFbKPewZI"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"5srpicvdelqq","executionInfo":{"status":"ok","timestamp":1734104976409,"user_tz":-60,"elapsed":635,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"outputs":[],"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","model = keras.Sequential([\n","layers.Dense(64, activation=\"relu\"),\n","layers.Dense(10, activation=\"softmax\")\n","])"]},{"cell_type":"markdown","source":["Incrementally building a Sequential model"],"metadata":{"id":"KesMFthTe07-"}},{"cell_type":"code","source":["model = keras.Sequential()\n","model.add(layers.Dense(64, activation=\"relu\"))\n","model.add(layers.Dense(10, activation=\"softmax\"))"],"metadata":{"id":"VByJznUGe2j4","executionInfo":{"status":"ok","timestamp":1734104977078,"user_tz":-60,"elapsed":41,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Models that aren’t yet built have no weights"],"metadata":{"id":"lxKhSLaEfC-g"}},{"cell_type":"code","source":["#model.weights"],"metadata":{"id":"u_eKEpK-fEFA","executionInfo":{"status":"ok","timestamp":1734104977078,"user_tz":-60,"elapsed":39,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Calling a model for the first time to build it"],"metadata":{"id":"vlEZQfHLfH4f"}},{"cell_type":"code","source":["model.build(input_shape=(None, 3))"],"metadata":{"id":"cdd_EhdPfJ5P","executionInfo":{"status":"ok","timestamp":1734104977078,"user_tz":-60,"elapsed":39,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Builds the model—now the model will expect samples of shape (3,). The\n","None in the input shape signals that the batch size could be anything."],"metadata":{"id":"RH56N6_9fcSm"}},{"cell_type":"code","source":["model.weights"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZPBB5ydEfLHe","executionInfo":{"status":"ok","timestamp":1734104977078,"user_tz":-60,"elapsed":38,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"780aa468-3f66-4036-9df6-666fb0495dd1"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<tf.Variable 'dense_6/kernel:0' shape=(3, 64) dtype=float32, numpy=\n"," array([[ 0.28205872, -0.09422328,  0.16054684,  0.14269537, -0.05271719,\n","          0.08404756, -0.16133088, -0.22974257, -0.13577159,  0.21715713,\n","         -0.26362568,  0.28449667,  0.152244  , -0.10567461, -0.01544565,\n","          0.00209969, -0.1059448 ,  0.09090069,  0.1977832 , -0.13646287,\n","         -0.12267064,  0.00480267,  0.08738619,  0.13446671, -0.19142729,\n","         -0.28652322,  0.21134895, -0.00335455,  0.11575827, -0.11606978,\n","         -0.06874803, -0.22469546,  0.0936226 ,  0.15409046, -0.29513612,\n","         -0.24406728,  0.21129441, -0.0668011 ,  0.24450684,  0.24574274,\n","          0.16207075, -0.15319876, -0.21082759,  0.04993513,  0.07678053,\n","          0.06792498, -0.25419855, -0.05054723, -0.28398517, -0.05555725,\n","         -0.0048604 ,  0.02152795, -0.27381545,  0.18184504,  0.190494  ,\n","         -0.07619685, -0.0640837 , -0.2923147 , -0.12619619, -0.21027657,\n","         -0.08317198, -0.2361535 ,  0.15281835, -0.20402968],\n","        [-0.03355649,  0.18084118, -0.27531388, -0.22321564, -0.0844354 ,\n","          0.2522915 ,  0.10510945,  0.16343683,  0.13358423,  0.18746495,\n","          0.07536736, -0.14472163,  0.19141951, -0.15956517, -0.07084322,\n","          0.13893214,  0.01795551, -0.07636593, -0.02841091,  0.05778015,\n","         -0.1677328 ,  0.06538156, -0.03362083, -0.11242899, -0.10249743,\n","          0.22383481, -0.11447917,  0.05204773,  0.01141009, -0.13401437,\n","          0.124605  ,  0.20956719,  0.06915677, -0.16061711, -0.25574836,\n","          0.05217651, -0.16804332,  0.05341983,  0.17399499,  0.09433165,\n","          0.23743099,  0.23343873, -0.1559679 ,  0.2717468 , -0.20288047,\n","         -0.2644157 , -0.23257244,  0.12871832,  0.21698779,  0.17915905,\n","         -0.07640389, -0.2720757 , -0.22714902,  0.05267137, -0.02834734,\n","          0.02294284,  0.11006567, -0.2080404 ,  0.04862064,  0.13507551,\n","          0.18955073, -0.03492978,  0.04627895, -0.10415326],\n","        [ 0.02846098,  0.2344302 , -0.1670252 , -0.14502592, -0.04367211,\n","         -0.0593826 ,  0.01886582,  0.12400684,  0.07639647,  0.00840816,\n","          0.24720901, -0.16615582, -0.10154472, -0.06980376,  0.03981236,\n","          0.25178093,  0.00678927, -0.17289366,  0.02381369,  0.23557949,\n","          0.06886697,  0.06208518, -0.12949294, -0.26941517, -0.23996267,\n","         -0.17306253, -0.16658947, -0.11050811, -0.23730062,  0.24363828,\n","         -0.10305558, -0.04965959,  0.21445405, -0.04353955, -0.18307772,\n","         -0.07993631,  0.01624575, -0.22295186,  0.05972406, -0.15207626,\n","          0.1005199 ,  0.07934004, -0.15015416, -0.05438265, -0.10391982,\n","         -0.14866543,  0.26487845, -0.23924455, -0.2790953 ,  0.2739886 ,\n","          0.14049372, -0.08260648, -0.23361546,  0.02289838,  0.01710361,\n","          0.0462026 , -0.21718186, -0.14262664, -0.0912829 , -0.2859946 ,\n","         -0.06585154,  0.22573692, -0.18181315, -0.2341298 ]],\n","       dtype=float32)>,\n"," <tf.Variable 'dense_6/bias:0' shape=(64,) dtype=float32, numpy=\n"," array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n"," <tf.Variable 'dense_7/kernel:0' shape=(64, 10) dtype=float32, numpy=\n"," array([[ 0.01240128,  0.1684871 , -0.16425468, -0.04022652, -0.19201472,\n","          0.02734435,  0.17074177, -0.2226756 ,  0.22128609,  0.07050917],\n","        [-0.2304157 ,  0.09711429, -0.12919392, -0.24316072, -0.21596768,\n","         -0.26351157, -0.08064312,  0.24248305, -0.2360577 , -0.22681725],\n","        [-0.15560813, -0.20572892,  0.12011349, -0.1609863 ,  0.23014018,\n","         -0.09426886, -0.1066485 ,  0.13369817, -0.25665835, -0.23347546],\n","        [ 0.04740572, -0.2381818 , -0.01941612,  0.27683786,  0.06019112,\n","          0.27741888,  0.27128753, -0.1566135 ,  0.17978281,  0.06359434],\n","        [-0.17919904,  0.23874435,  0.03007302,  0.08204395,  0.2839932 ,\n","         -0.24532937,  0.144467  ,  0.06604573,  0.16549617,  0.10437253],\n","        [-0.15815057,  0.21356124,  0.26457676,  0.16830134,  0.1104731 ,\n","          0.10736671,  0.05320773, -0.03813709, -0.1699892 , -0.10031173],\n","        [-0.0130032 , -0.13882907, -0.01977676, -0.15951107, -0.07905017,\n","         -0.08693236,  0.2332581 , -0.2836806 , -0.21921   , -0.2063719 ],\n","        [-0.13720225, -0.20736837, -0.04061273,  0.03843722, -0.09410368,\n","         -0.20497835,  0.00151148,  0.13354498, -0.1452839 ,  0.27263662],\n","        [-0.01868218,  0.02287218,  0.02556416, -0.17117658,  0.01500124,\n","         -0.09352188, -0.191748  , -0.02914873,  0.10088867,  0.07815859],\n","        [-0.12568548, -0.00581864, -0.26677063, -0.07031427, -0.16035351,\n","         -0.2600945 ,  0.02718651, -0.06217009, -0.21494745,  0.12731639],\n","        [ 0.21858343, -0.15032187,  0.01772374,  0.1827499 , -0.24924529,\n","          0.22458151, -0.02794096,  0.10338834,  0.06937814, -0.01964003],\n","        [-0.15672225,  0.1718575 ,  0.25008264,  0.2823945 , -0.12928855,\n","         -0.04780999,  0.12676886,  0.26084468,  0.16440058,  0.06744018],\n","        [-0.0932802 , -0.17273173, -0.08370973, -0.1679046 , -0.13121857,\n","         -0.14482   , -0.16697711,  0.2586889 , -0.22271498,  0.0160079 ],\n","        [ 0.04293337, -0.13347161, -0.22293474, -0.14094192,  0.2143721 ,\n","         -0.04946411, -0.00545874,  0.08742034, -0.06473936,  0.05818963],\n","        [ 0.0975346 , -0.1273062 , -0.06232964, -0.13795201,  0.12642792,\n","         -0.16608238,  0.01985592,  0.0298073 , -0.08574973, -0.04892738],\n","        [-0.12466313,  0.1442689 , -0.1320995 ,  0.27894518,  0.14272726,\n","         -0.07578647,  0.10992312,  0.04686472,  0.23867986, -0.18601842],\n","        [ 0.2045233 , -0.12007995, -0.17519182, -0.13707747, -0.21148863,\n","          0.0215427 ,  0.2828206 ,  0.26409945, -0.2790113 , -0.08572984],\n","        [-0.05214661,  0.18072253, -0.05744305, -0.00935227, -0.22265144,\n","         -0.03275281, -0.16675511,  0.06414574, -0.13328627, -0.19678454],\n","        [ 0.10948518,  0.09834939,  0.08603752,  0.05359536,  0.07847217,\n","         -0.22617644, -0.28442878, -0.11034594, -0.27522907, -0.02650511],\n","        [ 0.2659785 , -0.22865093,  0.18875673,  0.10157984, -0.25884128,\n","          0.10954717,  0.0026772 , -0.21248096,  0.07265237,  0.18215922],\n","        [ 0.09325039, -0.21271044, -0.2728869 , -0.09548944, -0.11714381,\n","         -0.06463046, -0.15116689,  0.18445873,  0.07433766,  0.07825989],\n","        [ 0.05887151,  0.25916168,  0.0687539 , -0.06898928, -0.1321065 ,\n","          0.07166663, -0.12738867,  0.04279551, -0.13077898, -0.0933385 ],\n","        [ 0.21478114,  0.02606568, -0.04336855, -0.00205949,  0.28368303,\n","         -0.01600301,  0.14826497,  0.04976749,  0.1478484 , -0.206492  ],\n","        [ 0.1424714 ,  0.25396875, -0.01309261,  0.00339064,  0.15311572,\n","         -0.19940826, -0.02730173, -0.08164197, -0.23846191, -0.23897114],\n","        [ 0.07832652, -0.04383013,  0.18906507,  0.14876741, -0.08625503,\n","          0.20703763, -0.1206443 ,  0.21632329, -0.04639946,  0.11483076],\n","        [-0.25453553, -0.20501766, -0.03555588,  0.07079682, -0.28310826,\n","          0.17524213,  0.10656804, -0.16241176,  0.04699928,  0.23160443],\n","        [ 0.02739173,  0.10993794, -0.00123498,  0.19885346,  0.27124384,\n","          0.18638521,  0.20808142,  0.14365482,  0.16439602, -0.00923848],\n","        [-0.25206295, -0.20669541,  0.11884516, -0.01163062,  0.20834416,\n","         -0.00746915,  0.08635461,  0.18063185,  0.14305577, -0.27092245],\n","        [-0.02433741, -0.20015028,  0.00446838, -0.06350037,  0.026898  ,\n","         -0.06657963,  0.18421862, -0.04358241, -0.2173106 ,  0.09291515],\n","        [ 0.181674  , -0.02392864,  0.01276526,  0.0461292 ,  0.26947448,\n","          0.15483487,  0.09847683,  0.10854486,  0.22341678, -0.22749077],\n","        [-0.2177768 ,  0.09636989, -0.07965077, -0.14607196,  0.08478135,\n","          0.06487507, -0.27602977, -0.03707497,  0.09179994,  0.26441363],\n","        [-0.19603187,  0.08364558, -0.01966718, -0.09931044,  0.24613771,\n","          0.06031597,  0.28065684, -0.01518387,  0.23212191,  0.08856067],\n","        [ 0.13574433,  0.03230155,  0.16562483, -0.07731865,  0.12492159,\n","          0.07462725, -0.00544116, -0.25128046,  0.15606591, -0.16940814],\n","        [ 0.16215047,  0.03349492,  0.02232853,  0.27566954,  0.05713138,\n","          0.10068426,  0.0072251 ,  0.26603958,  0.08301809, -0.07907081],\n","        [ 0.16458198,  0.09572726,  0.20418236,  0.2758362 ,  0.19043061,\n","         -0.18852061, -0.1930977 , -0.18515216,  0.07338217, -0.0693381 ],\n","        [-0.12923262, -0.04739587, -0.18902774,  0.07518587,  0.11764145,\n","          0.252543  ,  0.23440316, -0.1448748 ,  0.22486833,  0.06191498],\n","        [ 0.09512454, -0.27862602,  0.21882173,  0.09142321,  0.07464546,\n","         -0.26126572,  0.08530545, -0.06139216,  0.0294967 , -0.10230571],\n","        [-0.14405762,  0.2708741 , -0.03707314, -0.11297013, -0.20218933,\n","          0.18317243, -0.20261616,  0.23604831,  0.07796189, -0.07760379],\n","        [-0.14521356,  0.02096882,  0.27877817,  0.1672796 , -0.01534265,\n","          0.03023106,  0.0049392 , -0.11911049, -0.09590043,  0.03779867],\n","        [ 0.23608819,  0.28274104, -0.02993658, -0.15125345, -0.08314307,\n","          0.28135315, -0.16826767, -0.27713886,  0.05843309, -0.20893893],\n","        [ 0.08432534, -0.0787444 ,  0.06961223, -0.07025486, -0.1893763 ,\n","         -0.22889566,  0.21470624, -0.02358159,  0.06075066, -0.12089957],\n","        [-0.16353628,  0.28471407,  0.06298822, -0.20567021, -0.03368941,\n","          0.16356242, -0.12721244,  0.10990798,  0.23067823, -0.1324716 ],\n","        [ 0.05652273, -0.02733043, -0.21400577, -0.2527994 ,  0.14758337,\n","         -0.27095866, -0.03012958, -0.23950048,  0.16003627, -0.04658595],\n","        [-0.12472159, -0.03854842, -0.14027566,  0.19658813,  0.09659183,\n","          0.2521898 ,  0.11613512,  0.2221413 ,  0.24167284,  0.20060188],\n","        [-0.02310795, -0.127103  ,  0.11723608, -0.27568862, -0.13973017,\n","         -0.23327872, -0.15106778,  0.06663597,  0.27502123, -0.00058404],\n","        [ 0.21791986,  0.18004215, -0.014144  , -0.14549714, -0.18262473,\n","          0.16473854, -0.23654316, -0.25820583,  0.28124216,  0.06394458],\n","        [-0.19239321, -0.26229674, -0.18411562, -0.1137258 , -0.02101365,\n","          0.00078037, -0.08354239,  0.17167056,  0.07705605,  0.03659099],\n","        [-0.05904408,  0.10474217,  0.05576047,  0.13303217, -0.1890581 ,\n","         -0.14013492, -0.0965236 ,  0.23708478, -0.02708116, -0.17523825],\n","        [-0.10840452,  0.03767273, -0.12994613,  0.10403261,  0.18703392,\n","         -0.20258017, -0.20097303, -0.1865358 ,  0.18147033,  0.01255617],\n","        [-0.07028589,  0.07464424,  0.23413548,  0.1309591 ,  0.10303646,\n","         -0.24104978,  0.10204631,  0.05214757, -0.06906646,  0.15754363],\n","        [ 0.21987632,  0.28156564, -0.16472718, -0.09888884, -0.1442509 ,\n","         -0.17551409, -0.20965257,  0.04525858,  0.1143668 , -0.11686628],\n","        [ 0.09839422, -0.17307979, -0.01588279, -0.18616262, -0.20364323,\n","         -0.23424573,  0.0815734 ,  0.07722822, -0.21904476, -0.2080842 ],\n","        [-0.15669055,  0.03723273,  0.0929254 , -0.04082054,  0.06534615,\n","         -0.03133726,  0.00418761, -0.18454829, -0.07152793, -0.0599377 ],\n","        [-0.13114688,  0.02648598, -0.22770366, -0.16078256,  0.03260189,\n","          0.17732659,  0.19580606,  0.03838313, -0.23493406,  0.22182235],\n","        [-0.01143524,  0.01147237, -0.25852117,  0.26822188, -0.2547938 ,\n","         -0.12208681, -0.14986499,  0.07873461, -0.04896478,  0.14233175],\n","        [ 0.27627334,  0.00316662, -0.10704748,  0.03704402, -0.04104912,\n","         -0.12302327, -0.09779522, -0.19694191, -0.1522451 , -0.00836712],\n","        [-0.11144961, -0.22865832,  0.21459484,  0.1957038 , -0.24824181,\n","          0.19501367,  0.10890928, -0.26829422, -0.08182105, -0.10369207],\n","        [ 0.04965901, -0.01343912, -0.02587944,  0.14912418, -0.04019202,\n","         -0.2572073 , -0.03411344,  0.27063283, -0.04946363, -0.23271082],\n","        [ 0.26314726,  0.23603097,  0.26835665, -0.23761663,  0.01029184,\n","         -0.22819865, -0.20177773,  0.20637646, -0.15099838, -0.24954094],\n","        [ 0.00832525, -0.2717903 ,  0.27077278, -0.2057797 , -0.24618423,\n","         -0.24999538,  0.23275301, -0.01603466, -0.08489799,  0.21942636],\n","        [-0.26718584,  0.05432585,  0.23722246, -0.01632822,  0.28127006,\n","         -0.15321884, -0.11784917, -0.28069714, -0.17575678, -0.14749789],\n","        [ 0.03917077,  0.07195961, -0.07975045,  0.00263214, -0.20406483,\n","          0.28380898, -0.27502215, -0.24973911, -0.14343596, -0.04219876],\n","        [ 0.24786004, -0.1997086 , -0.04566619, -0.01370043, -0.05494955,\n","         -0.18511522,  0.14761215,  0.16671112,  0.2738051 ,  0.02327666],\n","        [ 0.15323046,  0.03179768,  0.0792937 , -0.22786172,  0.2210013 ,\n","          0.2766557 , -0.08923136,  0.07423425, -0.2634385 ,  0.19807172]],\n","       dtype=float32)>,\n"," <tf.Variable 'dense_7/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["The summary() method"],"metadata":{"id":"e9IZ64nwffX2"}},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"byFN70Cwffs2","executionInfo":{"status":"ok","timestamp":1734104977078,"user_tz":-60,"elapsed":36,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"b8b6e370-ebe3-4518-d846-682580f2c721"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_6 (Dense)             (None, 64)                256       \n","                                                                 \n"," dense_7 (Dense)             (None, 10)                650       \n","                                                                 \n","=================================================================\n","Total params: 906 (3.54 KB)\n","Trainable params: 906 (3.54 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["Naming models and layers with the name argument"],"metadata":{"id":"tHnGU7tTfo7w"}},{"cell_type":"code","source":["model = keras.Sequential(name=\"my_example_model\")\n","model.add(layers.Dense(64, activation=\"relu\", name=\"my_first_layer\"))\n","model.add(layers.Dense(10, activation=\"softmax\", name=\"my_last_layer\"))\n","model.build((None, 3))\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P1B8P-_qfqpm","executionInfo":{"status":"ok","timestamp":1734104977078,"user_tz":-60,"elapsed":17,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"e988d94e-26b3-4048-9b12-11031c290d88"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"my_example_model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," my_first_layer (Dense)      (None, 64)                256       \n","                                                                 \n"," my_last_layer (Dense)       (None, 10)                650       \n","                                                                 \n","=================================================================\n","Total params: 906 (3.54 KB)\n","Trainable params: 906 (3.54 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["Specifying the input shape of your model in advance.\n","Use Input to declare the shape\n","of the inputs. Note that the\n","shape argument must be the\n","shape of each sample, not\n","the shape of one batch."],"metadata":{"id":"g1GvFfl3f5ce"}},{"cell_type":"code","source":["model = keras.Sequential()\n","model.add(keras.Input(shape=(3,)))\n","model.add(layers.Dense(64, activation=\"relu\"))"],"metadata":{"id":"gLsS_hjof9En","executionInfo":{"status":"ok","timestamp":1734104977078,"user_tz":-60,"elapsed":10,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["use summary() to follow how the output shape of your model changes as\n","you add more layers:"],"metadata":{"id":"Xgmc_f8XgEH2"}},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XhxAPv81gFZe","executionInfo":{"status":"ok","timestamp":1734104977078,"user_tz":-60,"elapsed":9,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"db3d59d8-ef34-4918-e8a7-c2900e00c056"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_8 (Dense)             (None, 64)                256       \n","                                                                 \n","=================================================================\n","Total params: 256 (1.00 KB)\n","Trainable params: 256 (1.00 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.add(layers.Dense(10, activation=\"softmax\"))\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bpr9pxZFgOeo","executionInfo":{"status":"ok","timestamp":1734104977649,"user_tz":-60,"elapsed":576,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"ee323363-7553-4b19-ceb9-7e82178e4bef"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_8 (Dense)             (None, 64)                256       \n","                                                                 \n"," dense_9 (Dense)             (None, 10)                650       \n","                                                                 \n","=================================================================\n","Total params: 906 (3.54 KB)\n","Trainable params: 906 (3.54 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["##The Functional API"],"metadata":{"id":"dLlMtD-bgWgX"}},{"cell_type":"markdown","source":["\n","In practice, it’s pretty common to encounter models\n","with multiple inputs (say, an image and its metadata), multiple outputs (different\n","things you want to predict about the data), or a nonlinear topology.\n","In such cases, you’d build your model using the Functional API."],"metadata":{"id":"0iAAglTdhAGn"}},{"cell_type":"markdown","source":["he stack of two layers we used in the previous section. Its Functional API version looks like the following listing."],"metadata":{"id":"JQ6PJki8hImu"}},{"cell_type":"markdown","source":["A simple Functional model with two Dense layers"],"metadata":{"id":"yHsCymRohKNe"}},{"cell_type":"code","source":["inputs = keras.Input(shape=(3,), name=\"my_input\")\n","features = layers.Dense(64, activation=\"relu\")(inputs)\n","outputs = layers.Dense(10, activation=\"softmax\")(features)\n","model = keras.Model(inputs=inputs, outputs=outputs)"],"metadata":{"id":"Aiz3FRdggXbf","executionInfo":{"status":"ok","timestamp":1734104977649,"user_tz":-60,"elapsed":24,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["We started by declaring an Input (note that you can also give names to these input\n","objects, like everything else)"],"metadata":{"id":"ZqNYOcljhTrI"}},{"cell_type":"code","source":["inputs = keras.Input(shape=(3,), name=\"my_input\")"],"metadata":{"id":"q36CsNJ_hVNW","executionInfo":{"status":"ok","timestamp":1734104977649,"user_tz":-60,"elapsed":23,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["inputs.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aJAT3c28hZHG","executionInfo":{"status":"ok","timestamp":1734104977649,"user_tz":-60,"elapsed":23,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"73f2f216-c964-4ea1-b5a8-0689526c9712"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([None, 3])"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["The model will process batches where each sample\n","has shape (3,). The number of samples per batch is\n","variable (indicated by the None batch size)."],"metadata":{"id":"pFWEh6Jxhc53"}},{"cell_type":"code","source":["inputs.dtype"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g5OnmjYKhdlH","executionInfo":{"status":"ok","timestamp":1734104977649,"user_tz":-60,"elapsed":20,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"8f576264-f707-45d0-e027-bb0bad0c24b1"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tf.float32"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["These batches will have\n","dtype float32."],"metadata":{"id":"Ol3zc-lxhgFO"}},{"cell_type":"markdown","source":["We call such an object a symbolic tensor. It doesn’t contain any actual data, but it\n","encodes the specifications of the actual tensors of data that the model will see when\n","you use it. It stands for future tensors of data."],"metadata":{"id":"P61YSc7chlmm"}},{"cell_type":"code","source":["features = layers.Dense(64, activation=\"relu\")(inputs)"],"metadata":{"id":"42i_hHaRho9n","executionInfo":{"status":"ok","timestamp":1734104977649,"user_tz":-60,"elapsed":18,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["All Keras layers can be called both on real tensors of data and on these symbolic tensors. In the latter case, they return a new symbolic tensor, with updated shape and\n","dtype information:"],"metadata":{"id":"e1lQgp8Vhto2"}},{"cell_type":"code","source":["features.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pc7ZnLWLhusm","executionInfo":{"status":"ok","timestamp":1734104977649,"user_tz":-60,"elapsed":17,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"a54f181f-57d5-4511-b307-299ecd102008"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([None, 64])"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["After obtaining the final outputs, we instantiated the model by specifying its inputs\n","and outputs in the Model constructor:"],"metadata":{"id":"ml8DIk4Ph1iG"}},{"cell_type":"code","source":["outputs = layers.Dense(10, activation=\"softmax\")(features)\n","model = keras.Model(inputs=inputs, outputs=outputs)"],"metadata":{"id":"9x0-H2kThzK3","executionInfo":{"status":"ok","timestamp":1734104977649,"user_tz":-60,"elapsed":15,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u_NZFDeBh3M3","executionInfo":{"status":"ok","timestamp":1734104977649,"user_tz":-60,"elapsed":14,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"6597f1e8-8330-47bf-c88e-cfbd78282e2b"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," my_input (InputLayer)       [(None, 3)]               0         \n","                                                                 \n"," dense_12 (Dense)            (None, 64)                256       \n","                                                                 \n"," dense_13 (Dense)            (None, 10)                650       \n","                                                                 \n","=================================================================\n","Total params: 906 (3.54 KB)\n","Trainable params: 906 (3.54 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["###MULTI-INPUT, MULTI-OUTPUT MODELS"],"metadata":{"id":"kcoejFBZh8BO"}},{"cell_type":"markdown","source":["Unlike this toy model, most deep learning models don’t look like lists—they look like\n","graphs. They may, for instance, have multiple inputs or multiple outputs. It’s for this\n","kind of model that the Functional API really shines."],"metadata":{"id":"JFia0_WZiCUj"}},{"cell_type":"markdown","source":["Let’s say you’re building a system to rank customer support tickets by priority and\n","route them to the appropriate department. Your model has three inputs:\n"," The title of the ticket (text input)\n"," The text body of the ticket (text input)\n"," Any tags added by the user (categorical input, assumed here to be one-hot\n","encoded)\n","We can encode the text inputs as arrays of ones and zeros of size vocabulary_size\n","(see chapter 11 for detailed information about text encoding techniques).\n","Your model also has two outputs:\n"," The priority score of the ticket, a scalar between 0 and 1 (sigmoid output)\n"," The department that should handle the ticket (a softmax over the set of departments)"],"metadata":{"id":"Lp-McLc5nZyG"}},{"cell_type":"markdown","source":["A multi-input, multi-output Functional model"],"metadata":{"id":"1Mc3B88Enbnu"}},{"cell_type":"code","source":["vocabulary_size = 10000\n","num_tags = 100\n","num_departments = 4\n","\n","title = keras.Input(shape=(vocabulary_size,), name=\"title\") #Define model inputs.\n","text_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\")\n","tags = keras.Input(shape=(num_tags,), name=\"tags\")\n","\n","features = layers.Concatenate()([title, text_body, tags]) #Combine input features into a single tensor, features, byconcatenating them.\n","features = layers.Dense(64, activation=\"relu\")(features) #Apply an intermediate layer to recombine input features into richer representations.\n","\n","priority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features)\n","department = layers.Dense(num_departments, activation=\"softmax\", name=\"department\")(features)\n","\n","model = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department]) #Create the model by specifying its inputs and outputs"],"metadata":{"id":"FiE6GiwZh87j","executionInfo":{"status":"ok","timestamp":1734104977649,"user_tz":-60,"elapsed":7,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["####TRAINING A MULTI-INPUT, MULTI-OUTPUT MODEL"],"metadata":{"id":"vJMSwON5pdO3"}},{"cell_type":"markdown","source":["These lists of data should\n","be in the same order as the inputs you passed to the Model constructor."],"metadata":{"id":"qODEgCwEpbyG"}},{"cell_type":"code","source":["import numpy as np\n","num_samples = 1280\n","title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n","text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n","tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))\n","priority_data = np.random.random(size=(num_samples, 1))\n","department_data = np.random.randint(0, 2, size=(num_samples, num_departments))"],"metadata":{"id":"mg_uyIUQqGr3","executionInfo":{"status":"ok","timestamp":1734104977650,"user_tz":-60,"elapsed":7,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["\n","\n","model.compile(optimizer=\"rmsprop\",\n","loss=[\"mean_squared_error\", \"categorical_crossentropy\"],\n","metrics=[[\"mean_absolute_error\"], [\"accuracy\"]])\n","\n","model.fit([title_data, text_body_data, tags_data],\n","[priority_data, department_data],\n","epochs=1)\n","\n","model.evaluate([title_data, text_body_data, tags_data],\n","[priority_data, department_data])\n","\n","priority_preds, department_preds = model.predict(\n","[title_data, text_body_data, tags_data])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Sam3N4Apl4V","executionInfo":{"status":"ok","timestamp":1734104980960,"user_tz":-60,"elapsed":3317,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"8eeebf88-2963-4a3b-9c17-213452b07228"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["40/40 [==============================] - 2s 33ms/step - loss: 34.9156 - priority_loss: 0.3345 - department_loss: 34.5810 - priority_mean_absolute_error: 0.4989 - department_accuracy: 0.1914\n","40/40 [==============================] - 0s 6ms/step - loss: 42.4674 - priority_loss: 0.3411 - department_loss: 42.1264 - priority_mean_absolute_error: 0.5062 - department_accuracy: 0.2578\n","40/40 [==============================] - 0s 4ms/step\n"]}]},{"cell_type":"markdown","source":["If you don’t want to rely on input order (for instance, because you have many inputs\n","or outputs), you can also leverage the names you gave to the Input objects and the\n","output layers, and pass data via dictionaries."],"metadata":{"id":"fgNbDF4YpxOW"}},{"cell_type":"code","source":["model.compile(optimizer=\"rmsprop\",\n","loss={\"priority\": \"mean_squared_error\", \"department\":\n","\"categorical_crossentropy\"},\n","metrics={\"priority\": [\"mean_absolute_error\"], \"department\":\n","[\"accuracy\"]})\n","model.fit({\"title\": title_data, \"text_body\": text_body_data,\n","\"tags\": tags_data},\n","{\"priority\": priority_data, \"department\": department_data},\n","epochs=1)\n","model.evaluate({\"title\": title_data, \"text_body\": text_body_data,\n","\"tags\": tags_data},\n","{\"priority\": priority_data, \"department\": department_data})\n","priority_preds, department_preds = model.predict(\n","{\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"85Q72HyApxpO","executionInfo":{"status":"ok","timestamp":1734104984169,"user_tz":-60,"elapsed":3214,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"7f46557a-66b9-4637-9f1f-2dd5e817ae05"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["40/40 [==============================] - 2s 30ms/step - loss: 43.7822 - priority_loss: 0.3411 - department_loss: 43.4412 - priority_mean_absolute_error: 0.5062 - department_accuracy: 0.2125\n","40/40 [==============================] - 0s 4ms/step - loss: 47.5298 - priority_loss: 0.3411 - department_loss: 47.1887 - priority_mean_absolute_error: 0.5062 - department_accuracy: 0.1281\n","40/40 [==============================] - 0s 4ms/step\n"]}]},{"cell_type":"code","source":["keras.utils.plot_model(model, \"ticket_classifier.png\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7IqaEsxuiqm","executionInfo":{"status":"ok","timestamp":1734104984170,"user_tz":-60,"elapsed":29,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"ad34a420-fefa-4e7e-cd71-ba88a351438a"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"]}]},{"cell_type":"code","source":["keras.utils.plot_model(\n","model, \"ticket_classifier_with_shape_info.png\", show_shapes=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-Xt3jXRuuom","executionInfo":{"status":"ok","timestamp":1734104984170,"user_tz":-60,"elapsed":25,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"4924dfea-b4dd-485c-99b3-a61127fe7da5"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"]}]},{"cell_type":"markdown","source":["Retrieving the inputs or outputs of a layer in a Functional model"],"metadata":{"id":"XrO3ktM8vKuG"}},{"cell_type":"code","source":["model.layers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m6PZC9_2vLQf","executionInfo":{"status":"ok","timestamp":1734104984170,"user_tz":-60,"elapsed":20,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"3307b848-0bf8-48dd-f8d7-22832acf14ca"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<keras.src.engine.input_layer.InputLayer at 0x7b1f9e89d2a0>,\n"," <keras.src.engine.input_layer.InputLayer at 0x7b1f9e89d030>,\n"," <keras.src.engine.input_layer.InputLayer at 0x7b1f9e89c7c0>,\n"," <keras.src.layers.merging.concatenate.Concatenate at 0x7b1f9e86dfc0>,\n"," <keras.src.layers.core.dense.Dense at 0x7b18bc0d2380>,\n"," <keras.src.layers.core.dense.Dense at 0x7b18bc0d1810>,\n"," <keras.src.layers.core.dense.Dense at 0x7b1f9e910790>]"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["model.layers[3].input"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x-qZZAQQvOBf","executionInfo":{"status":"ok","timestamp":1734104984170,"user_tz":-60,"elapsed":17,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"8c20251a-5971-4cfe-8352-d65536dcb93f"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<KerasTensor: shape=(None, 10000) dtype=float32 (created by layer 'title')>,\n"," <KerasTensor: shape=(None, 10000) dtype=float32 (created by layer 'text_body')>,\n"," <KerasTensor: shape=(None, 100) dtype=float32 (created by layer 'tags')>]"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["model.layers[3].output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y3HQavvjvRSe","executionInfo":{"status":"ok","timestamp":1734104984170,"user_tz":-60,"elapsed":14,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"6cfe4c04-d6bd-4c0e-8014-febb532df037"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<KerasTensor: shape=(None, 20100) dtype=float32 (created by layer 'concatenate')>"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["Let’s say you want to add another output to the previous model—you want to estimate how long a given issue ticket will take to resolve, a kind of difficulty rating. You\n","could do this via a classification layer over three categories: “quick,” “medium,” and\n","“difficult.” You don’t need to recreate and retrain a model from scratch. You can start\n","from the intermediate features of your previous model, since you have access to them,\n","like this.\n","\n","Creating a new model by reusing intermediate layer outputs"],"metadata":{"id":"2NDsD5ASvb-P"}},{"cell_type":"code","source":["features = model.layers[4].output #layers[4] is our intermediate Dense layer\n","difficulty = layers.Dense(3, activation=\"softmax\", name=\"difficulty\")(features)\n","new_model = keras.Model(\n","inputs=[title, text_body, tags],\n","outputs=[priority, department, difficulty])"],"metadata":{"id":"Y05zxgwWvePg","executionInfo":{"status":"ok","timestamp":1734104984171,"user_tz":-60,"elapsed":12,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["keras.utils.plot_model(\n","new_model, \"updated_ticket_classifier.png\", show_shapes=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cE0-wOWRvrnv","executionInfo":{"status":"ok","timestamp":1734104984171,"user_tz":-60,"elapsed":12,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"65094cc6-f2e9-474f-ae03-1d63db6f2dc2"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"]}]},{"cell_type":"markdown","source":["##Subclassing the Model class"],"metadata":{"id":"_2MqUXIUvyl9"}},{"cell_type":"markdown","source":[" In the __init__() method, define the layers the model will use.\n"," In the call() method, define the forward pass of the model, reusing the layers\n","previously created.\n"," Instantiate your subclass, and call it on data to create its weights."],"metadata":{"id":"Hq_h6o-0yB0A"}},{"cell_type":"markdown","source":["**REWRITING OUR PREVIOUS EXAMPLE AS A SUBCLASSED MODEL**"],"metadata":{"id":"_e8ZhEMDyEcn"}},{"cell_type":"markdown","source":["A simple subclassed model"],"metadata":{"id":"FnaENh_kyL6e"}},{"cell_type":"code","source":["class CustomerTicketModel(keras.Model):\n","  def __init__(self, num_departments):\n","    super().__init__()\n","    self.concat_layer = layers.Concatenate()\n","    self.mixing_layer = layers.Dense(64, activation=\"relu\")\n","    self.priority_scorer = layers.Dense(1, activation=\"sigmoid\")\n","    self.department_classifier = layers.Dense(\n","      num_departments, activation=\"softmax\")\n","  def call(self, inputs):\n","    title = inputs[\"title\"]\n","    text_body = inputs[\"text_body\"]\n","    tags = inputs[\"tags\"]\n","    features = self.concat_layer([title, text_body, tags])\n","    features = self.mixing_layer(features)\n","    priority = self.priority_scorer(features)\n","    department = self.department_classifier(features)\n","    return priority, department"],"metadata":{"id":"ardPKDDFvzpb","executionInfo":{"status":"ok","timestamp":1734104984171,"user_tz":-60,"elapsed":8,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["Once you’ve defined the model, you can instantiate it. Note that it will only create its\n","weights the first time you call it on some data, much like Layer subclasses:"],"metadata":{"id":"MUSSZF62zDae"}},{"cell_type":"code","source":["model = CustomerTicketModel(num_departments=4)\n","priority, department = model(\n","  {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})"],"metadata":{"id":"vBZ_U3yuyz6b","executionInfo":{"status":"ok","timestamp":1734104985059,"user_tz":-60,"elapsed":896,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["a “layer” is a building block you use to create models, and a “model”\n","is the top-level object that you will actually train, export for inference, etc."],"metadata":{"id":"935oMRx2z1VP"}},{"cell_type":"markdown","source":["The structure of what you pass as the loss and\n","metrics arguments must match exactly what gets\n","returned by call()—here, a list of two elements.\n","\n","The structure of the target\n","data must match exactly what is\n","returned by the call() method—\n","here, a list of two elements.\n","\n","he structure of the input data must match\n","exactly what is expected by the call() method—\n","here, a dict with keys title, text_body, and tags."],"metadata":{"id":"n7IpiRlV0EL-"}},{"cell_type":"code","source":["model.compile(optimizer=\"rmsprop\",\n","loss=[\"mean_squared_error\", \"categorical_crossentropy\"],\n","metrics=[[\"mean_absolute_error\"], [\"accuracy\"]])\n","\n","model.fit({\"title\": title_data,\n","\"text_body\": text_body_data,\n","\"tags\": tags_data},\n","[priority_data, department_data],\n","epochs=1)\n","\n","model.evaluate({\"title\": title_data,\n","\"text_body\": text_body_data,\n","\"tags\": tags_data},\n","[priority_data, department_data])\n","\n","priority_preds, department_preds = model.predict({\"title\": title_data,\n","\"text_body\": text_body_data,\n","\"tags\": tags_data})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xxClsqlFz8lo","executionInfo":{"status":"ok","timestamp":1734104988367,"user_tz":-60,"elapsed":3311,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"ccc68545-f1c7-4535-fb22-0f3fbc6889ce"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["40/40 [==============================] - 2s 31ms/step - loss: 43.9599 - output_1_loss: 0.3358 - output_2_loss: 43.6241 - output_1_mean_absolute_error: 0.5017 - output_2_accuracy: 0.2336\n","40/40 [==============================] - 0s 6ms/step - loss: 38.6708 - output_1_loss: 0.3411 - output_2_loss: 38.3298 - output_1_mean_absolute_error: 0.5062 - output_2_accuracy: 0.0648\n","40/40 [==============================] - 0s 5ms/step\n"]}]},{"cell_type":"markdown","source":[" because the way layers are connected to each other is hidden inside\n","the body of the call() method, you cannot access that information. Calling summary() will not display layer connectivity, and you cannot plot the model topology via\n","plot_model(). Likewise, if you have a subclassed model, you cannot access the nodes\n","of the graph of layers to do feature extraction because there is simply no graph. Once\n","the model is instantiated, its forward pass becomes a complete black box."],"metadata":{"id":"8JqUBje20pk-"}},{"cell_type":"markdown","source":["**Mixing and matching different components**"],"metadata":{"id":"OVoDVZwC0r_m"}},{"cell_type":"markdown","source":["Creating a Functional model that includes a subclassed model"],"metadata":{"id":"X791mJu11M62"}},{"cell_type":"code","source":["class Classifier(keras.Model):\n","  def __init__(self, num_classes=2):\n","    super().__init__()\n","    if num_classes == 2:\n","      num_units = 1\n","      activation = \"sigmoid\"\n","    else:\n","      num_units = num_classes\n","      activation = \"softmax\"\n","    self.dense = layers.Dense(num_units, activation=activation)\n","\n","  def call(self, inputs):\n","    return self.dense(inputs)\n","\n","inputs = keras.Input(shape=(3,))\n","features = layers.Dense(64, activation=\"relu\")(inputs)\n","outputs = Classifier(num_classes=10)(features)\n","model = keras.Model(inputs=inputs, outputs=outputs)"],"metadata":{"id":"rli0HzNC0sgx","executionInfo":{"status":"ok","timestamp":1734104988367,"user_tz":-60,"elapsed":13,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["Creating a subclassed model that includes a Functional model"],"metadata":{"id":"iYoWZoCk1xSo"}},{"cell_type":"code","source":["inputs = keras.Input(shape=(64,))\n","outputs = layers.Dense(1, activation=\"sigmoid\")(inputs)\n","binary_classifier = keras.Model(inputs=inputs, outputs=outputs)\n","\n","class MyModel(keras.Model):\n","  def __init__(self, num_classes=2):\n","    super().__init__()\n","    self.dense = layers.Dense(64, activation=\"relu\")\n","    self.classifier = binary_classifier\n","    print(\"done\")\n","  def call(self, inputs):\n","    features = self.dense(inputs)\n","    return self.classifier(features)\n","\n","model = MyModel()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uVUiNEFV1x7p","executionInfo":{"status":"ok","timestamp":1734104988367,"user_tz":-60,"elapsed":12,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"33501e71-5d09-4a61-9dc9-5905e82a054d"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["done\n"]}]},{"cell_type":"markdown","source":["##Using built-in training and evaluation loops"],"metadata":{"id":"fxYs-YQV44Bm"}},{"cell_type":"markdown","source":["The standard workflow: compile(), fit(), evaluate(), predict()"],"metadata":{"id":"RD3Y4jeL5Hp9"}},{"cell_type":"code","source":["from tensorflow.keras.datasets import mnist\n","def get_mnist_model():\n","  inputs = keras.Input(shape=(28 * 28,))\n","  features = layers.Dense(512, activation=\"relu\")(inputs)\n","  features = layers.Dropout(0.5)(features)\n","  outputs = layers.Dense(10, activation=\"softmax\")(features)\n","  model = keras.Model(inputs, outputs)\n","  return model\n","(images, labels), (test_images, test_labels) = mnist.load_data()\n","images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n","test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n","train_images, val_images = images[10000:], images[:10000]\n","train_labels, val_labels = labels[10000:], labels[:10000]\n","model = get_mnist_model()\n","model.compile(optimizer=\"rmsprop\",\n","loss=\"sparse_categorical_crossentropy\",\n","metrics=[\"accuracy\"])\n","model.fit(train_images, train_labels,\n","epochs=3,\n","validation_data=(val_images, val_labels))\n","test_metrics = model.evaluate(test_images, test_labels)\n","predictions = model.predict(test_images)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SfDVIFvn1_p-","executionInfo":{"status":"ok","timestamp":1734105012717,"user_tz":-60,"elapsed":24360,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"35dbe9d6-1642-48ef-c904-0e00cbc80d06"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 0s 0us/step\n","Epoch 1/3\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.2934 - accuracy: 0.9127 - val_loss: 0.1489 - val_accuracy: 0.9570\n","Epoch 2/3\n","1563/1563 [==============================] - 7s 5ms/step - loss: 0.1578 - accuracy: 0.9538 - val_loss: 0.1246 - val_accuracy: 0.9653\n","Epoch 3/3\n","1563/1563 [==============================] - 7s 5ms/step - loss: 0.1298 - accuracy: 0.9628 - val_loss: 0.1074 - val_accuracy: 0.9707\n","313/313 [==============================] - 1s 2ms/step - loss: 0.1040 - accuracy: 0.9713\n","313/313 [==============================] - 1s 2ms/step\n"]}]},{"cell_type":"markdown","source":["There are a couple of ways you can customize this simple workflow:\n"," Provide your own custom metrics.\n"," Pass callbacks to the fit() method to schedule actions to be taken at specific\n","points during training\n","\n","**Writing your own metrics**"],"metadata":{"id":"lHd1aRBU52IK"}},{"cell_type":"markdown","source":["Implementing a custom metric by subclassing the Metric class"],"metadata":{"id":"MhI7WyIe6TN2"}},{"cell_type":"code","source":["import tensorflow as tf\n","class RootMeanSquaredError(keras.metrics.Metric):\n","  def __init__(self, name=\"rmse\", **kwargs):\n","    super().__init__(name=name, **kwargs)\n","    self.mse_sum = self.add_weight(name=\"mse_sum\", initializer=\"zeros\")\n","    self.total_samples = self.add_weight(\n","      name=\"total_samples\", initializer=\"zeros\", dtype=\"int32\")\n","\n","  def update_state(self, y_true, y_pred, sample_weight=None):\n","    y_true = tf.one_hot(y_true, depth=tf.shape(y_pred)[1])\n","    mse = tf.reduce_sum(tf.square(y_true - y_pred))\n","    self.mse_sum.assign_add(mse)\n","    num_samples = tf.shape(y_pred)[0]\n","    self.total_samples.assign_add(num_samples)\n","\n","  def result(self):\n","    return tf.sqrt(self.mse_sum / tf.cast(self.total_samples, tf.float32))\n","\n","  def reset_state(self):\n","    self.mse_sum.assign(0.)\n","    self.total_samples.assign(0)"],"metadata":{"id":"_Q2A96GH58iI","executionInfo":{"status":"ok","timestamp":1734105013116,"user_tz":-60,"elapsed":402,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["you also need to expose a way to reset the metric state without having to\n","reinstantiate it—this enables the same metric objects to be used across different\n","epochs of training or across both training and evaluation. You do this with the\n","reset_state() method:"],"metadata":{"id":"w0xZW0hR8SdW"}},{"cell_type":"code","source":["model = get_mnist_model()\n","model.compile(optimizer=\"rmsprop\",\n","loss=\"sparse_categorical_crossentropy\",\n","metrics=[\"accuracy\", RootMeanSquaredError()])\n","model.fit(train_images, train_labels,\n","epochs=3,\n","validation_data=(val_images, val_labels))\n","test_metrics = model.evaluate(test_images, test_labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D_UH5YEy8cSW","executionInfo":{"status":"ok","timestamp":1734105036825,"user_tz":-60,"elapsed":23713,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"358d8fa3-efad-4f4a-e76d-19f436c46e4e"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.2941 - accuracy: 0.9132 - rmse: 7.1773 - val_loss: 0.1523 - val_accuracy: 0.9551 - val_rmse: 7.3520\n","Epoch 2/3\n","1563/1563 [==============================] - 7s 5ms/step - loss: 0.1622 - accuracy: 0.9537 - rmse: 7.3508 - val_loss: 0.1107 - val_accuracy: 0.9684 - val_rmse: 7.4021\n","Epoch 3/3\n","1563/1563 [==============================] - 7s 5ms/step - loss: 0.1303 - accuracy: 0.9635 - rmse: 7.3838 - val_loss: 0.1037 - val_accuracy: 0.9709 - val_rmse: 7.4178\n","313/313 [==============================] - 1s 2ms/step - loss: 0.1011 - accuracy: 0.9715 - rmse: 7.4316\n"]}]},{"cell_type":"markdown","source":["The keras.callbacks module includes a number of built-in callbacks (this is not an\n","exhaustive list):\n","\n","keras.callbacks.ModelCheckpoint\n","\n","keras.callbacks.EarlyStopping\n","\n","keras.callbacks.LearningRateScheduler\n","\n","keras.callbacks.ReduceLROnPlateau\n","\n","keras.callbacks.CSVLogger\n"],"metadata":{"id":"NTlg2IIvI4t9"}},{"cell_type":"markdown","source":["**THE EARLYSTOPPING AND MODELCHECKPOINT CALLBACKS**"],"metadata":{"id":"zKWs_sE0JBxA"}},{"cell_type":"markdown","source":["A much better way to\n","handle this is to stop training when you measure that the validation loss is no longer\n","improving. This can be achieved using the EarlyStopping callback.\n","\n","The EarlyStopping callback interrupts training once a target metric being monitored has stopped improving for a fixed number of epochs."],"metadata":{"id":"t7ZR2hVIJR_0"}},{"cell_type":"markdown","source":["Using the callbacks argument in the fit() method\n","\n","Callbacks are passed to the model via the\n","callbacks argument in fit(), which takes a list of\n","callbacks. You can pass any number of callbacks.\n","\n","You monitor accuracy,\n","so it should be part of\n","the model’s metrics.\n","\n","Note that because the callback\n","will monitor validation loss and\n","validation accuracy, you need to pass\n","validation_data to the call to fit()"],"metadata":{"id":"iWotGM4tJewQ"}},{"cell_type":"code","source":["callbacks_list = [\n","keras.callbacks.EarlyStopping(\n","monitor=\"val_accuracy\",\n","patience=2,\n","),\n","keras.callbacks.ModelCheckpoint(\n","filepath=\"checkpoint_path.keras\",\n","monitor=\"val_loss\",\n","save_best_only=True,\n",")\n","]\n","\n","model = get_mnist_model()\n","model.compile(optimizer=\"rmsprop\",\n","loss=\"sparse_categorical_crossentropy\",\n","metrics=[\"accuracy\"])\n","model.fit(train_images, train_labels,\n","epochs=10,\n","callbacks=callbacks_list,\n","validation_data=(val_images, val_labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HQVD0rB2JCxK","executionInfo":{"status":"ok","timestamp":1734105110564,"user_tz":-60,"elapsed":73746,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"750cab42-8945-4438-d09a-0d190b86224c"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.2966 - accuracy: 0.9102 - val_loss: 0.1495 - val_accuracy: 0.9567\n","Epoch 2/10\n","1563/1563 [==============================] - 7s 5ms/step - loss: 0.1600 - accuracy: 0.9543 - val_loss: 0.1134 - val_accuracy: 0.9687\n","Epoch 3/10\n","1563/1563 [==============================] - 7s 5ms/step - loss: 0.1300 - accuracy: 0.9627 - val_loss: 0.1127 - val_accuracy: 0.9696\n","Epoch 4/10\n","1563/1563 [==============================] - 7s 5ms/step - loss: 0.1146 - accuracy: 0.9683 - val_loss: 0.0961 - val_accuracy: 0.9749\n","Epoch 5/10\n","1563/1563 [==============================] - 7s 5ms/step - loss: 0.1048 - accuracy: 0.9701 - val_loss: 0.0867 - val_accuracy: 0.9759\n","Epoch 6/10\n","1563/1563 [==============================] - 7s 5ms/step - loss: 0.0938 - accuracy: 0.9743 - val_loss: 0.0936 - val_accuracy: 0.9767\n","Epoch 7/10\n","1563/1563 [==============================] - 7s 5ms/step - loss: 0.0872 - accuracy: 0.9761 - val_loss: 0.0969 - val_accuracy: 0.9772\n","Epoch 8/10\n","1563/1563 [==============================] - 7s 5ms/step - loss: 0.0813 - accuracy: 0.9781 - val_loss: 0.0945 - val_accuracy: 0.9794\n","Epoch 9/10\n","1563/1563 [==============================] - 7s 5ms/step - loss: 0.0809 - accuracy: 0.9783 - val_loss: 0.0902 - val_accuracy: 0.9794\n","Epoch 10/10\n","1563/1563 [==============================] - 7s 5ms/step - loss: 0.0715 - accuracy: 0.9796 - val_loss: 0.0976 - val_accuracy: 0.9797\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7b17f83e8c70>"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["Note that you can always save models manually after training as well—just call\n","model.save('my_checkpoint_path'). To reload the model you’ve saved, just use"],"metadata":{"id":"kMHG3xavKOTP"}},{"cell_type":"code","source":["model = keras.models.load_model(\"checkpoint_path.keras\")"],"metadata":{"id":"Atz8Er8zKPm5","executionInfo":{"status":"ok","timestamp":1734105110564,"user_tz":-60,"elapsed":6,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":["**Writing your own callbacks**"],"metadata":{"id":"rYfmTW-gKTyw"}},{"cell_type":"markdown","source":["You can then implement any number\n","of the following transparently named methods, which are called at various points\n","during training:\n","\n","on_epoch_begin(epoch, logs)\n","\n","on_epoch_end(epoch, logs)\n","\n","on_batch_begin(batch, logs)\n","\n","on_batch_end(batch, logs)\n","\n","on_train_begin(logs)\n","\n","on_train_end(logs)"],"metadata":{"id":"DcfIZ25tKuih"}},{"cell_type":"markdown","source":["a logs argument, which is a dictionary containing\n","information about the previous batch, epoch, or training run—training and validation metrics, and so on."],"metadata":{"id":"y8_pHlvbK-wV"}},{"cell_type":"markdown","source":["Here’s a simple example that saves a list of per-batch loss values during training\n","and saves a graph of these values at the end of each epoch."],"metadata":{"id":"MhJXnS7lLKLm"}},{"cell_type":"markdown","source":["Creating a custom callback by subclassing the Callback class"],"metadata":{"id":"ud6Aw9ngLLeV"}},{"cell_type":"code","source":["from matplotlib import pyplot as plt\n","class LossHistory(keras.callbacks.Callback):\n","  def on_train_begin(self, logs):\n","    self.per_batch_losses = []\n","  def on_batch_end(self, batch, logs):\n","    self.per_batch_losses.append(logs.get(\"loss\"))\n","  def on_epoch_end(self, epoch, logs):\n","    plt.clf()\n","    plt.plot(range(len(self.per_batch_losses)), self.per_batch_losses,\n","    label=\"Training loss for each batch\")\n","    plt.xlabel(f\"Batch (epoch {epoch})\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.savefig(f\"plot_at_epoch_{epoch}\")\n","    self.per_batch_losses = []"],"metadata":{"id":"yACCVNskKuTD","executionInfo":{"status":"ok","timestamp":1734105110565,"user_tz":-60,"elapsed":6,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["model = get_mnist_model()\n","model.compile(optimizer=\"rmsprop\",\n","loss=\"sparse_categorical_crossentropy\",\n","metrics=[\"accuracy\"])\n","model.fit(train_images, train_labels,\n","epochs=10,\n","callbacks=[LossHistory()],\n","validation_data=(val_images, val_labels))"],"metadata":{"id":"nJ2KjYlpKUgX","executionInfo":{"status":"ok","timestamp":1734105188188,"user_tz":-60,"elapsed":77629,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"colab":{"base_uri":"https://localhost:8080/","height":813},"outputId":"72ac43d0-3b83-4c0c-dc33-ffea8f771fc3"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.2936 - accuracy: 0.9118 - val_loss: 0.1466 - val_accuracy: 0.9577\n","Epoch 2/10\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.1569 - accuracy: 0.9538 - val_loss: 0.1130 - val_accuracy: 0.9678\n","Epoch 3/10\n","1563/1563 [==============================] - 7s 5ms/step - loss: 0.1297 - accuracy: 0.9642 - val_loss: 0.0984 - val_accuracy: 0.9723\n","Epoch 4/10\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.1119 - accuracy: 0.9682 - val_loss: 0.0957 - val_accuracy: 0.9746\n","Epoch 5/10\n","1563/1563 [==============================] - 7s 5ms/step - loss: 0.1025 - accuracy: 0.9723 - val_loss: 0.0950 - val_accuracy: 0.9757\n","Epoch 6/10\n","1563/1563 [==============================] - 7s 5ms/step - loss: 0.0947 - accuracy: 0.9743 - val_loss: 0.0970 - val_accuracy: 0.9762\n","Epoch 7/10\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.0878 - accuracy: 0.9768 - val_loss: 0.0868 - val_accuracy: 0.9787\n","Epoch 8/10\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.0811 - accuracy: 0.9780 - val_loss: 0.0864 - val_accuracy: 0.9792\n","Epoch 9/10\n","1563/1563 [==============================] - 9s 6ms/step - loss: 0.0767 - accuracy: 0.9796 - val_loss: 0.0862 - val_accuracy: 0.9796\n","Epoch 10/10\n","1563/1563 [==============================] - 7s 5ms/step - loss: 0.0727 - accuracy: 0.9804 - val_loss: 0.0899 - val_accuracy: 0.9799\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7b178c7dcc40>"]},"metadata":{},"execution_count":45},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["###Monitoring and visualization with TensorBoard"],"metadata":{"id":"uNHA7c1ELqVI"}},{"cell_type":"markdown","source":["The easiest way to use TensorBoard with a Keras model and the fit() method is to\n","use the keras.callbacks.TensorBoard callback.\n","In the simplest case, just specify where you want the callback to write logs, and\n","you’re good to go:"],"metadata":{"id":"ycM6HrjBNZAe"}},{"cell_type":"code","source":["model = get_mnist_model()\n","model.compile(optimizer=\"rmsprop\",\n","loss=\"sparse_categorical_crossentropy\",\n","metrics=[\"accuracy\"])\n","\n","tensorboard = keras.callbacks.TensorBoard(\n","log_dir=\"/full_path_to_your_log_dir\",\n",")\n","\n","model.fit(train_images, train_labels,\n","epochs=10,\n","validation_data=(val_images, val_labels),\n","callbacks=[tensorboard])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L6CI-jrDLuzm","executionInfo":{"status":"ok","timestamp":1734105265496,"user_tz":-60,"elapsed":77315,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"ecaf0cd3-f5d1-4511-dc38-bc251b187885"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.2919 - accuracy: 0.9136 - val_loss: 0.1405 - val_accuracy: 0.9589\n","Epoch 2/10\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.1577 - accuracy: 0.9556 - val_loss: 0.1049 - val_accuracy: 0.9700\n","Epoch 3/10\n","1563/1563 [==============================] - 7s 5ms/step - loss: 0.1280 - accuracy: 0.9638 - val_loss: 0.1073 - val_accuracy: 0.9729\n","Epoch 4/10\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.1147 - accuracy: 0.9672 - val_loss: 0.0960 - val_accuracy: 0.9745\n","Epoch 5/10\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.1004 - accuracy: 0.9720 - val_loss: 0.0898 - val_accuracy: 0.9752\n","Epoch 6/10\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.0909 - accuracy: 0.9746 - val_loss: 0.0878 - val_accuracy: 0.9784\n","Epoch 7/10\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.0839 - accuracy: 0.9775 - val_loss: 0.0865 - val_accuracy: 0.9786\n","Epoch 8/10\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.0830 - accuracy: 0.9785 - val_loss: 0.0870 - val_accuracy: 0.9794\n","Epoch 9/10\n","1563/1563 [==============================] - 7s 5ms/step - loss: 0.0784 - accuracy: 0.9791 - val_loss: 0.0853 - val_accuracy: 0.9792\n","Epoch 10/10\n","1563/1563 [==============================] - 8s 5ms/step - loss: 0.0730 - accuracy: 0.9800 - val_loss: 0.0882 - val_accuracy: 0.9807\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7b1f9e912bf0>"]},"metadata":{},"execution_count":46}]},{"cell_type":"markdown","source":["If you are running your script in a Colab notebook, you can run an embedded TensorBoard instance as part of your notebook, using the following commands:\n","\n","%load_ext tensorboard\n","\n","%tensorboard --logdir /full_path_to_your_log_dir"],"metadata":{"id":"CDGt_vAqNwst"}},{"cell_type":"markdown","source":["##Writing your own training and evaluation loops"],"metadata":{"id":"FA_JYbLdN5j9"}},{"cell_type":"markdown","source":["The fit() workflow strikes a nice balance between ease of use and flexibility. It’s what\n","you will use most of the time. However, it isn’t meant to support everything a deep\n","learning researcher may want to do, even with custom metrics, custom losses, and custom callbacks.\n","After all, the built-in fit() workflow is solely focused on supervised learning: a setup\n","where there are known targets (also called labels or annotations) associated with your\n","input data, and where you compute your loss as a function of these targets and the\n","model’s predictions."],"metadata":{"id":"ebT6TCn-QV_t"}},{"cell_type":"markdown","source":["Whenever you find yourself in a situation where the built-in fit() is not enough,\n","you will need to write your own custom training logic. You already saw simple examples of low-level training loops in chapters 2 and 3. As a reminder, the contents of a\n","typical training loop look like this:\n","1 Run the forward pass (compute the model’s output) inside a gradient tape to\n","obtain a loss value for the current batch of data.\n","2 Retrieve the gradients of the loss with regard to the model’s weights.\n","3 Update the model’s weights so as to lower the loss value on the current batch\n","of data."],"metadata":{"id":"cUH9LErFQ3G9"}},{"cell_type":"markdown","source":["These steps are repeated for as many batches as necessary. This is essentially what\n","fit() does under the hood. In this section, you will learn to reimplement fit() from\n","scratch,"],"metadata":{"id":"DCNVx6CfQ5_P"}},{"cell_type":"markdown","source":["###Training versus inference"],"metadata":{"id":"m33EbMCWQ849"}},{"cell_type":"markdown","source":["By extension, Functional and Sequential models also\n","expose this training argument in their call() methods. Remember to pass training\n","=True when you call a Keras model during the forward pass! Our forward pass thus\n","becomes predictions = model(inputs, training=True)."],"metadata":{"id":"4zNwVpsPRfHQ"}},{"cell_type":"markdown","source":["In addition, note that when you retrieve the gradients of the weights of your\n","model, you should not use tape.gradients(loss, model.weights), but rather tape\n",".gradients(loss, model.trainable_weights). Indeed, layers and models own two\n","kinds of weights:\n"," Trainable weights—These are meant to be updated via backpropagation to minimize the loss of the model, such as the kernel and bias of a Dense layer.\n"," Non-trainable weights—These are meant to be updated during the forward pass\n","by the layers that own them. For instance, if you wanted a custom layer to keep\n","a counter of how many batches it has processed so far, that information would\n","be stored in a non-trainable weight, and at each batch, your layer would increment the counter by one."],"metadata":{"id":"w6A1tg6uRuKV"}},{"cell_type":"markdown","source":["Among Keras built-in layers, the only layer that features non-trainable weights is the\n","BatchNormalization layer, which we will discuss in chapter 9. The BatchNormalization\n","layer needs non-trainable weights in order to track information about the mean and\n","standard deviation of the data that passes through it, so as to perform an online\n","approximation of feature normalization (a concept you learned about in chapter 6).\n","Taking into account these two details, a supervised-learning training step ends up\n","looking like this:"],"metadata":{"id":"_EIyXw4LSCqt"}},{"cell_type":"code","source":["def train_step(inputs, targets):\n","  with tf.GradientTape() as tape:\n","    predictions = model(inputs, training=True)\n","    loss = loss_fn(targets, predictions)\n","    gradients = tape.gradients(loss, model.trainable_weights)\n","    optimizer.apply_gradients(zip(model.trainable_weights, gradients))"],"metadata":{"id":"lRB2f25ENzQu","executionInfo":{"status":"ok","timestamp":1734105266170,"user_tz":-60,"elapsed":682,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":["###Low-level usage of metrics"],"metadata":{"id":"3TJ2MYjCSXDc"}},{"cell_type":"markdown","source":["In a low-level training loop, you will probably want to leverage Keras metrics (whether\n","custom ones or the built-in ones). You’ve already learned about the metrics API: simply call update_state(y_true, y_pred) for each batch of targets and predictions, and\n","then use result() to query the current metric value:"],"metadata":{"id":"pOSckEhlTtht"}},{"cell_type":"code","source":["metric = keras.metrics.SparseCategoricalAccuracy()\n","targets = [0, 1, 2]\n","predictions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n","metric.update_state(targets, predictions)\n","current_result = metric.result()\n","print(f\"result: {current_result:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RTWdTyBBSXxm","executionInfo":{"status":"ok","timestamp":1734105266170,"user_tz":-60,"elapsed":16,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"15c12ad7-fbc4-4e58-a673-f12d805b8142"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["result: 1.00\n"]}]},{"cell_type":"markdown","source":["You may also need to track the average of a scalar value, such as the model’s loss. You\n","can do this via the keras.metrics.Mean metric:"],"metadata":{"id":"1rEUjoFITrDF"}},{"cell_type":"code","source":["values = [0, 1, 2, 3, 4]\n","mean_tracker = keras.metrics.Mean()\n","for value in values:\n","  mean_tracker.update_state(value)\n","print(f\"Mean of values: {mean_tracker.result():.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5GWlrcD1TxvV","executionInfo":{"status":"ok","timestamp":1734105266170,"user_tz":-60,"elapsed":13,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"dc644112-fe07-48ad-ceb2-932f4a518f0c"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean of values: 2.00\n"]}]},{"cell_type":"markdown","source":["Remember to use metric.reset_state() when you want to reset the current results\n","(at the start of a training epoch or at the start of evaluation)."],"metadata":{"id":"oRxV33TwT5F2"}},{"cell_type":"markdown","source":["###A complete training and evaluation loop"],"metadata":{"id":"njG2FHLHT7_l"}},{"cell_type":"markdown","source":["Writing a step-by-step training loop: the training step function"],"metadata":{"id":"q1TgwUcvWejW"}},{"cell_type":"code","source":["model = get_mnist_model()\n","loss_fn = keras.losses.SparseCategoricalCrossentropy()\n","optimizer = keras.optimizers.RMSprop()\n","metrics = [keras.metrics.SparseCategoricalAccuracy()]\n","loss_tracking_metric = keras.metrics.Mean()\n","\n","def train_step(inputs, targets):\n","  with tf.GradientTape() as tape:\n","    predictions = model(inputs, training=True)\n","    loss = loss_fn(targets, predictions)\n","  gradients = tape.gradient(loss, model.trainable_weights)\n","  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n","  logs = {}\n","  for metric in metrics:\n","    metric.update_state(targets, predictions)\n","    logs[metric.name] = metric.result()\n","  loss_tracking_metric.update_state(loss)\n","  logs[\"loss\"] = loss_tracking_metric.result()\n","  return logs"],"metadata":{"id":"DI2CfkqiWgm9","executionInfo":{"status":"ok","timestamp":1734105266171,"user_tz":-60,"elapsed":9,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":["We will need to reset the state of our metrics at the start of each epoch and before running evaluation. Here’s a utility function to do it."],"metadata":{"id":"xjqeR_gYYbJ8"}},{"cell_type":"markdown","source":["Writing a step-by-step training loop: resetting the metrics"],"metadata":{"id":"z_VfyDGUYcoF"}},{"cell_type":"code","source":["def reset_metrics():\n","  for metric in metrics:\n","    metric.reset_state()\n","  loss_tracking_metric.reset_state()"],"metadata":{"id":"t7VdWGkJYeGl","executionInfo":{"status":"ok","timestamp":1734105266171,"user_tz":-60,"elapsed":9,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":["We can now lay out our complete training loop. Note that we use a tf.data.Dataset\n","object to turn our NumPy data into an iterator that iterates over the data in batches of\n","size 32.\n","\n","Writing a step-by-step training loop: the loop itself"],"metadata":{"id":"xOsFloO-YsLl"}},{"cell_type":"markdown","source":["tf.data.Dataset.from_tensor_slices(): This line creates a TensorFlow Dataset object from your training data, train_images and train_labels. A Dataset is a way to efficiently feed data to your model during training. It takes the train_images (input data) and train_labels (target data) and creates a paired dataset.\n","\n",".batch(32): This divides the dataset into batches of 32 samples each. Training in batches is more efficient than processing individual samples."],"metadata":{"id":"G7wtKhbzZLTV"}},{"cell_type":"code","source":["training_dataset = tf.data.Dataset.from_tensor_slices(\n","(train_images, train_labels))\n","training_dataset = training_dataset.batch(32)\n","epochs = 3\n","\n","for epoch in range(epochs):\n","  reset_metrics()\n","  for inputs_batch, targets_batch in training_dataset:\n","    logs = train_step(inputs_batch, targets_batch)\n","  print(f\"Results at the end of epoch {epoch}\")\n","  for key, value in logs.items():\n","    print(f\"...{key}: {value:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X4J76nS3Ywme","executionInfo":{"status":"ok","timestamp":1734105350379,"user_tz":-60,"elapsed":84216,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"68812b75-ab76-448a-f040-ca5b9d377beb"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Results at the end of epoch 0\n","...sparse_categorical_accuracy: 0.9143\n","...loss: 0.2869\n","Results at the end of epoch 1\n","...sparse_categorical_accuracy: 0.9548\n","...loss: 0.1577\n","Results at the end of epoch 2\n","...sparse_categorical_accuracy: 0.9638\n","...loss: 0.1283\n"]}]},{"cell_type":"markdown","source":["And here’s the evaluation loop: a simple for loop that repeatedly calls a test_step()\n","function, which processes a single batch of data. The test_step() function is just a subset of the logic of train_step(). It omits the code that deals with updating the weights\n","of the model—that is to say, everything involving the GradientTape and the optimizer.\n","\n","Writing a step-by-step evaluation loop"],"metadata":{"id":"Ch9MWcudZ0au"}},{"cell_type":"code","source":["def test_step(inputs, targets):\n","  predictions = model(inputs, training=False)\n","  loss = loss_fn(targets, predictions)\n","\n","  logs = {}\n","  for metric in metrics:\n","    metric.update_state(targets, predictions)\n","    logs[\"val_\" + metric.name] = metric.result()\n","  loss_tracking_metric.update_state(loss)\n","  logs[\"val_loss\"] = loss_tracking_metric.result()\n","  return logs\n","\n","val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n","val_dataset = val_dataset.batch(32)\n","reset_metrics()\n","\n","for inputs_batch, targets_batch in val_dataset:\n","  logs = test_step(inputs_batch, targets_batch)\n","print(\"Evaluation results:\")\n","for key, value in logs.items():\n","  print(f\"...{key}: {value:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TSWeD4AYZ4Rl","executionInfo":{"status":"ok","timestamp":1734105352460,"user_tz":-60,"elapsed":2089,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"9a679dae-9cc8-4aa6-cb16-d21375621585"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluation results:\n","...val_sparse_categorical_accuracy: 0.9703\n","...val_loss: 0.1122\n"]}]},{"cell_type":"markdown","source":["###Make it fast with tf.function"],"metadata":{"id":"zdPaKZlWeqRe"}},{"cell_type":"markdown","source":["It’s more performant to compile your TensorFlow code into a computation graph that\n","can be globally optimized in a way that code interpreted line by line cannot. The syntax to do this is very simple: just add a @tf.function to any function you want to compile before executing, as shown in the following listing."],"metadata":{"id":"6uiyrs_re7qB"}},{"cell_type":"markdown","source":["Adding a @tf.function decorator to our evaluation-step function"],"metadata":{"id":"MJxMSG26fAcP"}},{"cell_type":"code","source":["@tf.function\n","def test_step(inputs, targets):\n","  predictions = model(inputs, training=False)\n","  loss = loss_fn(targets, predictions)\n","  logs = {}\n","  for metric in metrics:\n","    metric.update_state(targets, predictions)\n","    logs[\"val_\" + metric.name] = metric.result()\n","  loss_tracking_metric.update_state(loss)\n","  logs[\"val_loss\"] = loss_tracking_metric.result()\n","  return logs\n","\n","val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n","val_dataset = val_dataset.batch(32)\n","reset_metrics()\n","\n","for inputs_batch, targets_batch in val_dataset:\n","  logs = test_step(inputs_batch, targets_batch)\n","print(\"Evaluation results:\")\n","for key, value in logs.items():\n","  print(f\"...{key}: {value:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fLXdIX0aerZr","executionInfo":{"status":"ok","timestamp":1734105353401,"user_tz":-60,"elapsed":947,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"3f96d988-3928-40a1-96fb-5497479af023"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluation results:\n","...val_sparse_categorical_accuracy: 0.9703\n","...val_loss: 0.1122\n"]}]},{"cell_type":"markdown","source":["Remember, while you are debugging your code, prefer running it eagerly, without\n","any @tf.function decorator. It’s easier to track bugs this way. Once your code is working and you want to make it fast, add a @tf.function decorator to your training step\n","and your evaluation step—or any other performance-critical function"],"metadata":{"id":"VBUoJbdGgjH_"}},{"cell_type":"markdown","source":["###Leveraging fit() with a custom training loop"],"metadata":{"id":"v16dFw8Ngk52"}},{"cell_type":"markdown","source":["e were writing our own training loop entirely from scratch.\n","Doing so provides you with the most flexibility, but you end up writing a lot of code\n","while simultaneously missing out on many convenient features of fit(), such as callbacks or built-in support for distributed training."],"metadata":{"id":"eb7dVormg0wq"}},{"cell_type":"markdown","source":["What if you need a custom training algorithm, but you still want to leverage the\n","power of the built-in Keras training logic? There’s actually a middle ground between\n","fit() and a training loop written from scratch: you can provide a custom training\n","step function and let the framework do the rest.\n","You can do this by overriding the train_step() method of the Model class. This is\n","the function that is called by fit() for every batch of data. You will then be able to call\n","fit() as usual, and it will be running your own learning algorithm under the hood.\n","Here’s a simple example:\n"," We create a new class that subclasses keras.Model.\n"," We override the method train_step(self, data). Its contents are nearly identical to what we used in the previous section. It returns a dictionary mapping\n","metric names (including the loss) to their current values.\n"," We implement a metrics property that tracks the model’s Metric instances.\n","This enables the model to automatically call reset_state() on the model’s\n","metrics at the start of each epoch and at the start of a call to evaluate(), so you\n","don’t have to do it by hand."],"metadata":{"id":"rcPcq2aThIHI"}},{"cell_type":"markdown","source":["Implementing a custom training step to use with fit()"],"metadata":{"id":"pRPTxnbdhKWZ"}},{"cell_type":"code","source":["loss_fn = keras.losses.SparseCategoricalCrossentropy()\n","loss_tracker = keras.metrics.Mean(name=\"loss\")\n","class CustomModel(keras.Model):\n","\n","  def train_step(self, data):\n","    inputs, targets = data\n","    with tf.GradientTape() as tape:\n","      predictions = self(inputs, training=True)\n","      loss = loss_fn(targets, predictions)\n","    gradients = tape.gradient(loss, model.trainable_weights)\n","    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n","    loss_tracker.update_state(loss)\n","    return {\"loss\": loss_tracker.result()}\n","  @property\n","  def metrics(self):\n","    return [loss_tracker]"],"metadata":{"id":"J0ALBBWIglqY","executionInfo":{"status":"ok","timestamp":1734105894873,"user_tz":-60,"elapsed":355,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":["We can now instantiate our custom model, compile it (we only pass the optimizer, since\n","the loss is already defined outside of the model), and train it using fit() as usual:"],"metadata":{"id":"sHyrjP5XuMT2"}},{"cell_type":"code","source":["'''inputs = keras.Input(shape=(28 * 28,))\n","features = layers.Dense(512, activation=\"relu\")(inputs)\n","features = layers.Dropout(0.5)(features)\n","outputs = layers.Dense(10, activation=\"softmax\")(features)\n","model = CustomModel(inputs, outputs)\n","model.compile(optimizer=keras.optimizers.RMSprop())\n","model.fit(train_images, train_labels, epochs=3)'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"1aIOuu0TuRNO","executionInfo":{"status":"ok","timestamp":1734106004444,"user_tz":-60,"elapsed":610,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"a9edba76-d20e-4a62-feb1-064ed170c2e9"},"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'inputs = keras.Input(shape=(28 * 28,))\\nfeatures = layers.Dense(512, activation=\"relu\")(inputs)\\nfeatures = layers.Dropout(0.5)(features)\\noutputs = layers.Dense(10, activation=\"softmax\")(features)\\nmodel = CustomModel(inputs, outputs)\\nmodel.compile(optimizer=keras.optimizers.RMSprop())\\nmodel.fit(train_images, train_labels, epochs=3)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":65}]},{"cell_type":"code","source":["class CustomModel(keras.Model):\n","  def train_step(self, data):\n","    inputs, targets = data\n","    with tf.GradientTape() as tape:\n","      predictions = self(inputs, training=True)\n","      loss = self.compiled_loss(targets, predictions)\n","    gradients = tape.gradient(loss, model.trainable_weights)\n","    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n","    self.compiled_metrics.update_state(targets, predictions)\n","    return {m.name: m.result() for m in self.metrics}"],"metadata":{"id":"HFwugqp0xbZE","executionInfo":{"status":"ok","timestamp":1734106877226,"user_tz":-60,"elapsed":538,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["inputs = keras.Input(shape=(28 * 28,))\n","features = layers.Dense(512, activation=\"relu\")(inputs)\n","features = layers.Dropout(0.5)(features)\n","outputs = layers.Dense(10, activation=\"softmax\")(features)\n","model = CustomModel(inputs, outputs)\n","model.compile(optimizer=keras.optimizers.RMSprop(),\n","loss=keras.losses.SparseCategoricalCrossentropy(),\n","metrics=[keras.metrics.SparseCategoricalAccuracy()])\n","model.fit(train_images, train_labels, epochs=3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":755},"id":"_5HIPIQZ0zHN","executionInfo":{"status":"error","timestamp":1734106912407,"user_tz":-60,"elapsed":608,"user":{"displayName":"Hayder Chakroun","userId":"02691572659768110935"}},"outputId":"84679409-e8e8-4136-e3bd-ae24d29b369c"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n"]},{"output_type":"error","ename":"KeyError","evalue":"in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"<ipython-input-66-542f52ce2b65>\", line 8, in train_step\n        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1223, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 652, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1253, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1345, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1342, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 233, in _update_step\n        raise KeyError(\n\n    KeyError: 'The optimizer cannot recognize variable dense_42/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.RMSprop.'\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-67-8478ea6c919d>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m metrics=[keras.metrics.SparseCategoricalAccuracy()])\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-66-542f52ce2b65>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"<ipython-input-66-542f52ce2b65>\", line 8, in train_step\n        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1223, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 652, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1253, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1345, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1342, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 233, in _update_step\n        raise KeyError(\n\n    KeyError: 'The optimizer cannot recognize variable dense_42/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.RMSprop.'\n"]}]}]}